<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[特征工程 (一) ---特征构建]]></title>
    <url>%2F2019%2F04%2F24%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E4%B8%80-%E7%89%B9%E5%BE%81%E6%9E%84%E5%BB%BA%2F</url>
          <content type="text"><![CDATA[特征工程就是一个从原始数据提取特征的过程，目标是使这些特征能表征数据的本质特点，使基于这些特征建立的模型在未知数据上的性能，可以达到最优、最大限度地减少“垃圾进，垃圾出”。 特征构建指的是从原始数据中构建新的特征，在实际应用中需要手工构建。特征构建针对时间型、数字型、文本型等不同种类的输入数据，结合数据的特点，通过分解或切分的方法基于原来的特征创建新的特征，从而提高数据的预测能力。 1. 单列变量 单列变量方法指基于单列变量生成，即对单个变量进行转换、衍生，单列变量按照类型可以分为字符型、数值型、时间(日期)型，按照变量的样本内容格式可以分为离散型和连须型。]]></content>
          <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[matplotlib ---多子图]]></title>
    <url>%2F2019%2F04%2F24%2Fmatplotlib-%E5%A4%9A%E5%AD%90%E5%9B%BE%2F</url>
          <content type="text"><![CDATA[1. subplot(numRows, numCols, plotNum) 12345678# 分成2x2，占用第一个，即第一行第一列的子图plt.subplot(221)# 分成2x2，占用第二个，即第一行第二列的子图plt.subplot(222)# 分成2x2，占用第二个，即第二行第一列的子图plt.subplot(223)plt.show() 2. subplots(nrows, ncols, sharex, sharey) 1234# 一行两列的子图，并返回[ax1，ax2]数组作为两个子图取值帮助f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)ax1.plot(x, y)ax2.plot(x, y) 3. GridSpec(nrows, ncols, wspace, hspace) 123456# 带行列间距的2 x 3网格grid = plt.GridSpec(2, 3, wspace=0.4, hspace=0.3)# 通过类似Python切片语法设置子图位置和扩展尺寸plt.subplot(grid[0, 0])plt.subplot(grid[0, 1:]) 4. subplot2grid(shape, loc, rowspan, colspan) 12345# 2 x 3网格 并选择第一行第一个网格plt.subplot2grid((2,3),(0,0))# 2 x 3网格 并选择第二行第一个网格并占两个网格位置大小plt.subplot2grid((2,3),(1,0), colspan=2)]]></content>
          <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模型的评估与数据处理 ---数据清洗]]></title>
    <url>%2F2019%2F04%2F23%2F%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%2F</url>
          <content type="text"><![CDATA[一、分析数据 二、缺失值处理 直接删除----适合缺失值数量较小，并且是随机出现的，删除它们对整体数据影响不大的情况； 使用一个全局常量填充—譬如将缺失值用“Unknown”等填充，但是效果不一定好，因为算法可能会把它识别为一个新的类别，一般很少用； 使用均值或中位数代替----优点：不会减少样本信息，处理简单。缺点：当缺失数据不是随机数据时会产生偏差.对于正常分布的数据可以使用均值代替，如果数据是倾斜的，使用中位数可能更好； 插补法： i. 随机插补法----从总体中随机抽取某个样本代替缺失样本; ii. 多重插补法----通过变量之间的关系对缺失数据进行预测，利用蒙特卡洛方法生成多个完整的数据集，在对这些数据集进行分析，最后对分析结果进行汇总处理; iii. 热平台插补----指在非缺失数据集中找到一个与缺失值所在样本相似的样本（匹配样本），利用其中的观测值对缺失值进行插补： 优点： 简单易行，准去率较高 缺点：变量数量较多时，通常很难找到与需要插补样本完全相同的样本。但我们可以按照某些变量将数据分层，在层中对缺失值实用均值插补 iv. 拉格朗日差值法和牛顿插值法（简单高效，数值分析里的内容，数学公式?） 建模法： 可以用回归、使用贝叶斯形式化方法的基于推理的工具或决策树归纳确定。例如，利用数据集中其他数据的属性，可以构造一棵判定树，来预测缺失值的值。 以上方法各有优缺点，具体情况要根据实际数据分布情况、倾斜程度、缺失值所占比例等等来选择方法。一般而言，建模法是比较常用的方法，它根据已有的值来预测缺失值，准确率更高。 三、异常值处理 异常值我们通常也称为“离群点”。在讲分析数据时，我们举了个例子说明如何发现离群点，除了画图（画图其实并不常用，因为数据量多时不好画图，而且慢），还有很多其他方法: 简单的统计分析 拿到数据后可以对数据进行一个简单的描述性统计分析，譬如最大最小值可以用来判断这个变量的取值是否超过了合理的范围，如客户的年龄为-20岁或200岁，显然是不合常理的，为异常值。 在python中可以直接用pandas的describe()函数。 使用3∂原则 如果数据服从正态分布，在3∂原则下，异常值为一组测定值中与平均值的偏差超过3倍标准差的值。如果数据服从正态分布，距离平均值3∂之外的值出现的概率为P(|x-u| &gt; 3∂) &lt;= 0.003，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。 箱型图分析 箱型图提供了识别异常值的一个标准：如果一个值小于QL01.5IQR或大于OU-1.5IQR的值，则被称为异常值。QL为下四分位数，表示全部观察值中有四分之一的数据取值比它小；QU为上四分位数，表示全部观察值中有四分之一的数据取值比它大；IQR为四分位数间距，是上四分位数QU与下四分位数QL的差值，包含了全部观察值的一半。箱型图判断异常值的方法以四分位数和四分位距为基础，四分位数具有鲁棒性：25%的数据可以变得任意远并且不会干扰四分位数，所以异常值不能对这个标准施加影响。因此箱型图识别异常值比较客观，在识别异常值时有一定的优越性。 基于模型检测 首先建立一个数据模型，异常是那些同模型不能完美拟合的对象；如果模型是簇的集合，则异常是不显著属于任何簇的对象；在使用回归模型时，异常是相对远离预测值的对象。优缺点： 有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效； 对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。 基于距离 通常可以在对象之间定义邻近性度量，异常对象是那些远离其他对象的对象。优缺点： 简单; 缺点：基于邻近度的方法需要O(m2)O(m^2)O(m2)时间，大数据集不适用; 该方法对参数的选择敏感; 不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。 基于密度 当一个点的局部密度显著低于它的大部分近邻时才将其分类为离群点。适合非均匀分布的数据。优缺点： .给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理； 与基于距离的方法一样，这些方法必然具有O(m2)O(m^2)O(m2)的时间复杂度。对于低维数据使用特定的数据结构可以达到O(mlog(m))O(m^{log(m)})O(mlog(m))； 参数选择困难。虽然算法通过观察不同的k值，取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。 基于聚类： 基于聚类的离群点：一个对象是基于聚类的离群点，如果该对象不强属于任何簇。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。 优缺点： 基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的； 簇的定义通常是离群点的补，因此可能同时发现簇和离群点； 产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性； 聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。 处理方法： 删除异常值----明显看出是异常且数量较少可以直接删除； 不处理—如果算法对异常值不敏感则可以不处理，但如果算法对异常值敏感，则最好不要用，如基于距离计算的一些算法，包括kmeans，knn之类的； 平均值替代----损失信息小，简单高效； 视为缺失值----可以按照处理缺失值的方法来处理。 四、去重处理 DataFrame的duplicated方法返回一个布尔型Series，表示各行是否是重复行； drop_duplicates方法用于返回一个移除了重复行的DataFrame； 五、噪音处理 噪音，是被测量变量的随机误差或方差。我们在上文中提到过异常点（离群点），那么离群点和噪音是不是一回事呢？我们知道，观测量(Measurement) = 真实数据(True Data) + 噪声 (Noise)。离群点(Outlier)属于观测量，既有可能是真实数据产生的，也有可能是噪声带来的，但是总的来说是和大部分观测量之间有明显不同的观测值。。噪音包括错误值或偏离期望的孤立点值，但也不能说噪声点包含离群点，虽然大部分数据挖掘方法都将离群点视为噪声或异常而丢弃。然而，在一些应用（例如：欺诈检测），会针对离群点做离群点分析或异常挖掘。而且有些点在局部是属于离群点，但从全局看是正常的。 噪音的处理方法： 分箱法 分箱方法通过考察数据的“近邻”（即，周围的值）来光滑有序数据值。这些有序的值被分布到一些“桶”或箱中。由于分箱方法考察近邻的值，因此它进行局部光滑。 用箱均值光滑：箱中每一个值被箱中的平均值替换。 用箱中位数平滑：箱中的每一个值被箱中的中位数替换。 用箱边界平滑：箱中的最大和最小值同样被视为边界。箱中的每一个值被最近的边界值替换。 一般而言，宽度越大，光滑效果越明显。箱也可以是等宽的，其中每个箱值的区间范围是个常量。分箱也可以作为一种离散化技术使用。 回归法 可以用一个函数拟合数据来光滑数据。线性回归涉及找出拟合两个属性（或变量）的“最佳”直线，使得一个属性能够预测另一个。多线性回归是线性回归的扩展，它涉及多于两个属性，并且数据拟合到一个多维面。使用回归，找出适合数据的数学方程式，能够帮助消除噪声。 参考文献： http://www.cnblogs.com/charlotte77/p/5606926.html]]></content>
          <categories>
        <category>machine learning</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning Note]]></title>
    <url>%2F2019%2F04%2F22%2Fmachine-learning-note%2F</url>
          <categories>
        <category>machine learning</category>
      </categories>
  </entry>
</search>
