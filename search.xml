<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[对数据进行归一化(标准化)]]></title>
    <url>%2F2020%2F01%2F26%2F%E5%AF%B9%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%BD%92%E4%B8%80%E5%8C%96-%E6%A0%87%E5%87%86%E5%8C%96%2F</url>
          <content type="text"><![CDATA[归一化旨在将行和列对齐并转化为一致的规则 标准化取保所有的行和列在机器学习中得到平等的对待，让数据的处理保持一致 zzz分数标准化 zzz分数标准化的输出会被重新缩放，使均值为0、标准差为1 z=(x−u)/σz = (x - u)/\sigma z=(x−u)/σ 在这个公式中： zzz是新的值 xxx是单元格原来的值 uuu是该列的均值 σ\sigmaσ是该列的标准差 min-max标准化 m=(x−xmin)/(xmax−xmin)m = (x - x_{min})/(x_{max} - x_{min}) m=(x−xmin​)/(xmax​−xmin​) 每列的所有值都会缩放位于0～1，副作用是标准差会非常小，异常值的权重降低了 行归一化 行归一化会保证每行有单位范数，意味着每行的向量长度相同 L2范数：∥x∥=(x12+x22+⋯+xn2)\left \| x \right \| = \sqrt{(x_1^2 + x_2^2 + \cdots + x_n^2 )}∥x∥=(x12​+x22​+⋯+xn2​)​]]></content>
          <categories>
        <category>Machine Learning</category>
        <category>Feature Engineering</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[缺失值处理]]></title>
    <url>%2F2020%2F01%2F26%2F%E7%BC%BA%E5%A4%B1%E5%80%BC%E5%A4%84%E7%90%86%2F</url>
          <content type="text"><![CDATA[清洗数据：调整已有的行和列 增强数据：在数据集中删除和添加新的列 识别数据中的缺失值 空准率：指当模型总是预测频率较高的类别时达到的正确率==&gt;serie.value_counts(normalize=True) 查看数据是否缺失：df.isnull().sum() 如果没有文档，缺失值的常见填充方法： 0（数值型） unknown或Unknown（类别型） ？（类别行） 处理数据中的缺失值 删除有害数据 删除存在缺失值的行，留下完整数据点df.dropna() 查看删除数据后对原模型预测准确率的影响 输入（填充）缺失值 填充指的是利用现有的知识/数据来确定缺失的数量值并填充的行为df.fillna(value, inplace = True) 查看填充缺失值后对原模型预测准确率的影响 在机器学习流水线中填充值 机器学习流水线：指的是在被解读为最终输出之前，原始数据不仅仅进入一种学习算法，而且会经过各种预处理步骤，乃至多种学习算法。 如果测试集从含有缺失值的数据中选取，则不能使用训练集和测试集一起计算来的均值来填充测试集和训练集，因为不能使用测试集数据计算得来的值训练模型 用训练集的的均值填充训练集和测试集的缺失值 scikit-learn的Pipeline和Imputer]]></content>
          <categories>
        <category>Machine Learning</category>
        <category>Feature Engineering</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[数据等级]]></title>
    <url>%2F2020%2F01%2F26%2F%E6%95%B0%E6%8D%AE%E7%AD%89%E7%BA%A7%2F</url>
          <content type="text"><![CDATA[定类等级(nominal level) 这个等级的数据只按名称分类。例如，血型（A、B、O和AB型）。 执行计数操作，统计使用众数，一般绘制条形图和饼状图 定序等级(ordinal level) 定序等级继承了定类等级的所有属性，而且有附加属性自然排序。例如，考试的成绩（F、D、C、B、A） 除了可以绘制条形图和饼状图外，还可以绘制箱线图 箱线图的绘制方法是：先找出一组数据的上边缘、上四分位数Q3、中位数、下四分位数Q1、下边缘、还有一个异常值；然后， 连接两个四分位数画出箱体；再将上边缘和下边缘与箱体相连接，中位数在箱体中间 箱形图为我们提供了识别异常值的一个标准：异常值被定义为小于Q1－1.5IQR或大于Q3+1.5IQR的值（其中四分位距IQR=Q3-Q1） 定距等级(interval level) 不仅可以对值进行排序和比较，而且可以进行加减，例如，温度 统计一般使平均值和标准差，可以绘制直方图（hist）、线图和散点图 定比等级(ratio level) 不仅继承了定距等级的加减运算，而且有一个绝对零点的概念，可以做乘除运算。 金钱是存在零点的，而温度没有真正零点]]></content>
          <categories>
        <category>Machine Learning</category>
        <category>Feature Engineering</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[python基础]]></title>
    <url>%2F2019%2F11%2F19%2Fpython%E5%9F%BA%E7%A1%80%2F</url>
          <content type="text"><![CDATA[python、numpy和pandas的数据结构 python–&gt;List是有序可重复，元素类型可以不相同，在内存中不连续 python–&gt;Set是无序不可重复，元素类型可以不相同 numpy–&gt;array是有序可重复，元素类型必须相同，在内存中连续 pandas–&gt;series的字符串表现形式为：索引在左边，值在右边。由于我们没有为数据指定索引。于是会自动创建一个0到N-1（N为长度）的整数型索引 pandas–&gt;DataFrame是一个表格型的数据结构(类Excel)，它包含有一组有序的列，每列可以是不同的值类型（数值，字符串，布尔值等）。DataFrame既有行索引也有列索引， 它可以被看做由Series组成的大字典 pandas–&gt;DataFrame是Series的容器，Panel是DataFrame的容器;panel的3轴(axis)的一些语义: items - axis 0，每个项目对应于内部包含的数据帧(DataFrame)。 major_axis - axis 1，它是每个数据帧(DataFrame)的索引(行)。 minor_axis - axis 2，它是每个数据帧(DataFrame)的列。 Pandas函数应用 apply()是一种让函数作用于列或者行操作，applymap()是一种让函数作用于DataFrame每一个元素的操作，而map是一种让函数作用于Series每一个元素的操作]]></content>
          <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[特征工程 (一) ---特征构建]]></title>
    <url>%2F2019%2F04%2F24%2F%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E4%B8%80-%E7%89%B9%E5%BE%81%E6%9E%84%E5%BB%BA%2F</url>
          <content type="text"><![CDATA[特征工程就是一个从原始数据提取特征的过程，目标是使这些特征能表征数据的本质特点，使基于这些特征建立的模型在未知数据上的性能，可以达到最优、最大限度地减少“垃圾进，垃圾出”。 特征构建指的是从原始数据中构建新的特征，在实际应用中需要手工构建。特征构建针对时间型、数字型、文本型等不同种类的输入数据，结合数据的特点，通过分解或切分的方法基于原来的特征创建新的特征，从而提高数据的预测能力。 1. 单列变量 单列变量方法指基于单列变量生成，即对单个变量进行转换、衍生，单列变量按照类型可以分为字符型、数值型、时间(日期)型，按照变量的样本内容格式可以分为离散型和连须型。]]></content>
          <categories>
        <category>Machine Learning</category>
        <category>Feature Engineering</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[matplotlib ---多子图]]></title>
    <url>%2F2019%2F04%2F24%2Fmatplotlib-%E5%A4%9A%E5%AD%90%E5%9B%BE%2F</url>
          <content type="text"><![CDATA[1. subplot(numRows, numCols, plotNum) 12345678# 分成2x2，占用第一个，即第一行第一列的子图plt.subplot(221)# 分成2x2，占用第二个，即第一行第二列的子图plt.subplot(222)# 分成2x2，占用第二个，即第二行第一列的子图plt.subplot(223)plt.show() 2. subplots(nrows, ncols, sharex, sharey) 1234# 一行两列的子图，并返回[ax1，ax2]数组作为两个子图取值帮助f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)ax1.plot(x, y)ax2.plot(x, y) 3. GridSpec(nrows, ncols, wspace, hspace) 123456# 带行列间距的2 x 3网格grid = plt.GridSpec(2, 3, wspace=0.4, hspace=0.3)# 通过类似Python切片语法设置子图位置和扩展尺寸plt.subplot(grid[0, 0])plt.subplot(grid[0, 1:]) 4. subplot2grid(shape, loc, rowspan, colspan) 12345# 2 x 3网格 并选择第一行第一个网格plt.subplot2grid((2,3),(0,0))# 2 x 3网格 并选择第二行第一个网格并占两个网格位置大小plt.subplot2grid((2,3),(1,0), colspan=2)]]></content>
          <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[模型的评估与数据处理 ---数据清洗]]></title>
    <url>%2F2019%2F04%2F23%2F%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%84%E4%BC%B0%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97%2F</url>
          <content type="text"><![CDATA[一、分析数据 二、缺失值处理 直接删除----适合缺失值数量较小，并且是随机出现的，删除它们对整体数据影响不大的情况； 使用一个全局常量填充—譬如将缺失值用“Unknown”等填充，但是效果不一定好，因为算法可能会把它识别为一个新的类别，一般很少用； 使用均值或中位数代替----优点：不会减少样本信息，处理简单。缺点：当缺失数据不是随机数据时会产生偏差.对于正常分布的数据可以使用均值代替，如果数据是倾斜的，使用中位数可能更好； 插补法： i. 随机插补法----从总体中随机抽取某个样本代替缺失样本; ii. 多重插补法----通过变量之间的关系对缺失数据进行预测，利用蒙特卡洛方法生成多个完整的数据集，在对这些数据集进行分析，最后对分析结果进行汇总处理; iii. 热平台插补----指在非缺失数据集中找到一个与缺失值所在样本相似的样本（匹配样本），利用其中的观测值对缺失值进行插补： 优点： 简单易行，准确率较高 缺点：变量数量较多时，通常很难找到与需要插补样本完全相同的样本。但我们可以按照某些变量将数据分层，在层中对缺失值实用均值插补 iv. 拉格朗日插值法和牛顿插值法（简单高效，数值分析里的内容，数学公式?） 建模法： 可以用回归、使用贝叶斯形式化方法的基于推理的工具或决策树归纳确定。例如，利用数据集中其他数据的属性，可以构造一棵判定树，来预测缺失值的值。 以上方法各有优缺点，具体情况要根据实际数据分布情况、倾斜程度、缺失值所占比例等等来选择方法。一般而言，建模法是比较常用的方法，它根据已有的值来预测缺失值，准确率更高。 三、异常值处理 异常值我们通常也称为“离群点”。在讲分析数据时，我们举了个例子说明如何发现离群点，除了画图（画图其实并不常用，因为数据量多时不好画图，而且慢），还有很多其他方法: 简单的统计分析 拿到数据后可以对数据进行一个简单的描述性统计分析，譬如最大最小值可以用来判断这个变量的取值是否超过了合理的范围，如客户的年龄为-20岁或200岁，显然是不合常理的，为异常值。 在python中可以直接用pandas的describe()函数。 使用3∂原则 如果数据服从正态分布，在3∂原则下，异常值为一组测定值中与平均值的偏差超过3倍标准差的值。如果数据服从正态分布，距离平均值3∂之外的值出现的概率为P(|x-u| &gt; 3∂) &lt;= 0.003，属于极个别的小概率事件。如果数据不服从正态分布，也可以用远离平均值的多少倍标准差来描述。 箱型图分析 箱型图提供了识别异常值的一个标准：如果一个值小于QL01.5IQR或大于OU-1.5IQR的值，则被称为异常值。QL为下四分位数，表示全部观察值中有四分之一的数据取值比它小；QU为上四分位数，表示全部观察值中有四分之一的数据取值比它大；IQR为四分位数间距，是上四分位数QU与下四分位数QL的差值，包含了全部观察值的一半。箱型图判断异常值的方法以四分位数和四分位距为基础，四分位数具有鲁棒性：25%的数据可以变得任意远并且不会干扰四分位数，所以异常值不能对这个标准施加影响。因此箱型图识别异常值比较客观，在识别异常值时有一定的优越性。 基于模型检测 首先建立一个数据模型，异常是那些同模型不能完美拟合的对象；如果模型是簇的集合，则异常是不显著属于任何簇的对象；在使用回归模型时，异常是相对远离预测值的对象。优缺点： 有坚实的统计学理论基础，当存在充分的数据和所用的检验类型的知识时，这些检验可能非常有效； 对于多元数据，可用的选择少一些，并且对于高维数据，这些检测可能性很差。 基于距离 通常可以在对象之间定义邻近性度量，异常对象是那些远离其他对象的对象。优缺点： 简单; 缺点：基于邻近度的方法需要O(m2)O(m^2)O(m2)时间，大数据集不适用; 该方法对参数的选择敏感; 不能处理具有不同密度区域的数据集，因为它使用全局阈值，不能考虑这种密度的变化。 基于密度 当一个点的局部密度显著低于它的大部分近邻时才将其分类为离群点。适合非均匀分布的数据。优缺点： .给出了对象是离群点的定量度量，并且即使数据具有不同的区域也能够很好的处理； 与基于距离的方法一样，这些方法必然具有O(m2)O(m^2)O(m2)的时间复杂度。对于低维数据使用特定的数据结构可以达到O(mlog(m))O(m^{log(m)})O(mlog(m))； 参数选择困难。虽然算法通过观察不同的k值，取得最大离群点得分来处理该问题，但是，仍然需要选择这些值的上下界。 基于聚类： 基于聚类的离群点：一个对象是基于聚类的离群点，如果该对象不强属于任何簇。离群点对初始聚类的影响：如果通过聚类检测离群点，则由于离群点影响聚类，存在一个问题：结构是否有效。为了处理该问题，可以使用如下方法：对象聚类，删除离群点，对象再次聚类（这个不能保证产生最优结果）。 优缺点： 基于线性和接近线性复杂度（k均值）的聚类技术来发现离群点可能是高度有效的； 簇的定义通常是离群点的补，因此可能同时发现簇和离群点； 产生的离群点集和它们的得分可能非常依赖所用的簇的个数和数据中离群点的存在性； 聚类算法产生的簇的质量对该算法产生的离群点的质量影响非常大。 处理方法： 删除异常值----明显看出是异常且数量较少可以直接删除； 不处理—如果算法对异常值不敏感则可以不处理，但如果算法对异常值敏感，则最好不要用，如基于距离计算的一些算法，包括kmeans，knn之类的； 平均值替代----损失信息小，简单高效； 视为缺失值----可以按照处理缺失值的方法来处理。 四、去重处理 DataFrame的duplicated方法返回一个布尔型Series，表示各行是否是重复行； drop_duplicates方法用于返回一个移除了重复行的DataFrame； 五、噪音处理 噪音，是被测量变量的随机误差或方差。我们在上文中提到过异常点（离群点），那么离群点和噪音是不是一回事呢？我们知道，观测量(Measurement) = 真实数据(True Data) + 噪声 (Noise)。离群点(Outlier)属于观测量，既有可能是真实数据产生的，也有可能是噪声带来的，但是总的来说是和大部分观测量之间有明显不同的观测值。。噪音包括错误值或偏离期望的孤立点值，但也不能说噪声点包含离群点，虽然大部分数据挖掘方法都将离群点视为噪声或异常而丢弃。然而，在一些应用（例如：欺诈检测），会针对离群点做离群点分析或异常挖掘。而且有些点在局部是属于离群点，但从全局看是正常的。 噪音的处理方法： 分箱法 分箱方法通过考察数据的“近邻”（即，周围的值）来光滑有序数据值。这些有序的值被分布到一些“桶”或箱中。由于分箱方法考察近邻的值，因此它进行局部光滑。 用箱均值光滑：箱中每一个值被箱中的平均值替换。 用箱中位数平滑：箱中的每一个值被箱中的中位数替换。 用箱边界平滑：箱中的最大和最小值同样被视为边界。箱中的每一个值被最近的边界值替换。 一般而言，宽度越大，光滑效果越明显。箱也可以是等宽的，其中每个箱值的区间范围是个常量。分箱也可以作为一种离散化技术使用。 回归法 可以用一个函数拟合数据来光滑数据。线性回归涉及找出拟合两个属性（或变量）的“最佳”直线，使得一个属性能够预测另一个。多线性回归是线性回归的扩展，它涉及多于两个属性，并且数据拟合到一个多维面。使用回归，找出适合数据的数学方程式，能够帮助消除噪声。 参考文献： http://www.cnblogs.com/charlotte77/p/5606926.html]]></content>
          <categories>
        <category>Machine Learning</category>
        <category>Feature Engineering</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning Note]]></title>
    <url>%2F2019%2F04%2F22%2Fmachine-learning-note%2F</url>
          <categories>
        <category>Machine Learning</category>
      </categories>
  </entry>
</search>
