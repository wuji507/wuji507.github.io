<!DOCTYPE html>












  


<html class="theme-next pisces use-motion" lang>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.1.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.1.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.1.0">


  <link rel="mask-icon" href="/images/logo.svg?v=7.1.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.1.0',
    sidebar: {"position":"right","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="第一章 初识机器学习1.1 机器学习与生活应用学习算法的实际建议，知道如何使用机器学习算法 花大量时间讨论，假设你确实想要开发机器学习系统，如何让那些最佳实践操作，指导你决定用什么方式建立自己的系统 1.2 什么是机器学习Arthur Samuel(1959): 在没有明确的设置情况下，使计算机具有学习能力的研究领域 Tom Mitchell(1998): 一个适当的学习问题定义如下：计算机程序从">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达机器学习">
<meta property="og:url" content="http://yoursite.com/2019/04/22/吴恩达机器学习/index.html">
<meta property="og:site_name" content="blog">
<meta property="og:description" content="第一章 初识机器学习1.1 机器学习与生活应用学习算法的实际建议，知道如何使用机器学习算法 花大量时间讨论，假设你确实想要开发机器学习系统，如何让那些最佳实践操作，指导你决定用什么方式建立自己的系统 1.2 什么是机器学习Arthur Samuel(1959): 在没有明确的设置情况下，使计算机具有学习能力的研究领域 Tom Mitchell(1998): 一个适当的学习问题定义如下：计算机程序从">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/1.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/2.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/3.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/4.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/5.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/6.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/7.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/8.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/9.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/10.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/11.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/12.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/13.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/14.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/15.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/16.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/17.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/18.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/19.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/20.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/26.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/21.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/22.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/23.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/24.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/25.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/27.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/28.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/31.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/29.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/30.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/32.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/33.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/34.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/35.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/36.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/37.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/38.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/39.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/40.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/41.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/42.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/43.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/44.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/45.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/46.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/47.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/48.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/49.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/50.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/51.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/52.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/53.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/54.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/55.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/56.png">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/57.png%27Photo%20OCR%20pipeline%27">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/58.png%27上限分析%27">
<meta property="og:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/59.png%27总结%27">
<meta property="og:updated_time" content="2019-04-21T19:30:46.013Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="吴恩达机器学习">
<meta name="twitter:description" content="第一章 初识机器学习1.1 机器学习与生活应用学习算法的实际建议，知道如何使用机器学习算法 花大量时间讨论，假设你确实想要开发机器学习系统，如何让那些最佳实践操作，指导你决定用什么方式建立自己的系统 1.2 什么是机器学习Arthur Samuel(1959): 在没有明确的设置情况下，使计算机具有学习能力的研究领域 Tom Mitchell(1998): 一个适当的学习问题定义如下：计算机程序从">
<meta name="twitter:image" content="http://yoursite.com/2019/04/22/吴恩达机器学习/images/1.png">





  
  
  <link rel="canonical" href="http://yoursite.com/2019/04/22/吴恩达机器学习/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>吴恩达机器学习 | blog</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/22/吴恩达机器学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="wuji">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/killer.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">吴恩达机器学习

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-04-22 03:28:14 / Modified: 03:30:46" itemprop="dateCreated datePublished" datetime="2019-04-22T03:28:14+08:00">2019-04-22</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="第一章-初识机器学习"><a href="#第一章-初识机器学习" class="headerlink" title="第一章 初识机器学习"></a>第一章 初识机器学习</h3><h4 id="1-1-机器学习与生活"><a href="#1-1-机器学习与生活" class="headerlink" title="1.1 机器学习与生活"></a>1.1 机器学习与生活</h4><p>应用学习算法的实际建议，知道如何使用机器学习算法</p>
<p>花大量时间讨论，假设你确实想要开发机器学习系统，如何让那些<strong>最佳实践操作</strong>，指导你决定用什么方式建立自己的系统</p>
<h4 id="1-2-什么是机器学习"><a href="#1-2-什么是机器学习" class="headerlink" title="1.2 什么是机器学习"></a>1.2 什么是机器学习</h4><p>Arthur Samuel(1959): 在没有明确的设置情况下，使计算机具有学习能力的研究领域</p>
<p>Tom Mitchell(1998): 一个适当的学习问题定义如下：计算机程序从<strong>经验E</strong>中学习，解决<strong>某一任务T</strong>进行某一<strong>性能度量P</strong>，通过P测定在T上的表现因经验E而提高</p>
<h4 id="1-3-监督学习-Supervised-Learning"><a href="#1-3-监督学习-Supervised-Learning" class="headerlink" title="1.3 监督学习(Supervised Learning)"></a>1.3 监督学习(Supervised Learning)</h4><p>通过算法对一些离散的数据进行一个监督。</p>
<p>定义：我们给算法一个数据集，其中包含了正确答案</p>
<p>回归问题：想要预测<strong>连续</strong>的具体数值输出 eg: 房价</p>
<p>==分类==问题：设法预测一个离散值输出 eg：乳腺癌(良性、恶性、正常)</p>
<p>支持向量机允许计算机处理<strong>无穷多的特征</strong></p>
<h4 id="1-4-无监督学习-Unsupervised-Learning"><a href="#1-4-无监督学习-Unsupervised-Learning" class="headerlink" title="1.4 无监督学习(Unsupervised Learning)"></a>1.4 无监督学习(Unsupervised Learning)</h4><p>通过给予算法大量的数据，来让算法找出数据的类型结构。</p>
<p>定义：给定的数据集都具有相同的标签或者没有任何标签。</p>
<p>==聚类==算法(Clustering)：把数据集划分成多个不同的簇(cluster). eg: google news,鸡尾酒会声音聚类</p>
<p>==先用<strong>Octave</strong>建立学习算法原型–&gt;算法原型工作后–&gt;迁移到C++ Java或其它编译环境==</p>
<hr>
<h3 id="第二章-单变量线性回归"><a href="#第二章-单变量线性回归" class="headerlink" title="第二章 单变量线性回归"></a>第二章 单变量线性回归</h3><h4 id="2-1-模型描述"><a href="#2-1-模型描述" class="headerlink" title="2.1 模型描述"></a>2.1 模型描述</h4><p>为了更加准确描述监督学习问题，我们给出一个训练集，由此来构建一个<strong>模型</strong>（即：函数）。</p>
<p>常见符号(Notation)：</p>
<blockquote>
<p>$x$表示输入变量，$y$表示输出变量</p>
<p>( $x$,$y$)是一个<strong>训练样例</strong>(training example), 其中($x^{(i)},y^{(i)}$)是第$i$个训练样例</p>
<p>由训练样例组成的集合就是<strong>训练集</strong>(training set)</p>
<p>$m$表示训练样本数量</p>
</blockquote>
<p><img src="images/1.png" alt="假设函数" title="假设函数"></p>
<p>假设函数是抽象出来的数据模型，含有大量的参数变量。</p>
<p>使用某种学习算法对训练集的数据进行训练, 我们可以得到<strong>假设函数</strong>(Hypothesis Function), 如上图所示. 在房价的例子中，假设函数就是一个房价关于房子面积的函数。有了这个假设函数之后, 给定一个房子的面积我们就可以预测它的价格了.</p>
<p>我们使用如下的形式表示假设函数, 为了方便$h_{\theta}(x)$也可以记作$h(x)$</p>
<p>$$h_{\theta}(x) = \theta_0 + \theta_{1}x \tag{2.1}$$</p>
<p>以上这个模型就叫做单变量的线性回归(Linear Regression with One Variable). (Linear regression with one variable = Univariate linear regression，univariate是one variable的装逼写法.)</p>
<h4 id="2-2-代价函数"><a href="#2-2-代价函数" class="headerlink" title="2.2 代价函数"></a>2.2 代价函数</h4><p>我们可以通过代价函数<strong>来衡量假设函数的准确性</strong>。这取决于$x$的输入和实际输出$y$的假设的所有结果的平均差异（实际上是平均值的平均）。</p>
<p>代价函数的目的本质是为了<strong>寻找到合适的模型参数</strong>，从而得到更合适的模型，进行更准确的预测。</p>
<p>模型即函数，模型参数即函数中的系数。</p>
<p>==<strong><em>模型的训练</em></strong>的本质是选择合适的模型参数，合理参数是根据代价函数最小化得到的。==（模型的训练就是通过最小化（最大化）代价函数求解模型参数）</p>
<blockquote>
<p>目标：寻找一个$\theta_0, \theta_1$ 使得预测值接近训练集中的样本</p>
<p>方法：预测值与ground truth求差的平方，对于所有样本的该值进行求和，最后除以$2m$（1/2为简化计算），形成代价函数。最小化该代价函数，得到对应的$\theta_0$和$\theta_1$.</p>
<p>平方误差——解决回归问题的最常用手段</p>
</blockquote>
<p>代价函数(cost function)：</p>
<p>$$ J(\theta_0,\theta_1) = \frac{1}{2m} \sum_{i=0}^{m} {(h_\theta(x^{(i)})-y^{(i)})^{2}} \tag{2.2} $$</p>
<p>我们的优化目标, 让代价函数最小:</p>
<p>$$\underset{\theta_0,\theta_1}{minimize}J(\theta_0,\theta_1) = a\theta_0^2 + b\theta_1^2 + c\theta_0\theta_1 + d\theta_0 + f\theta_1 + g \tag{2.3} $$</p>
<p>假设函数与代价函数：<br>    （1）假设函数是$x$的函数<br>    （2）代价函数是模型参数的函数</p>
<p>代价函数(含$\theta_0, \theta_1$ 两个参数)：</p>
<ul>
<li>只含一个参数$\theta_1$的图像是一条抛物线(二次函数)，含$\theta_0, \theta_1$ 两个参数的图像是椭圆抛物面；</li>
</ul>
<p><img src="images/2.png" alt="椭圆抛物面" title="椭圆抛物面"></p>
<ul>
<li>等高线图(contour plots/figures)是==平面==($J(\theta_0,\theta_1) = 某高度如上图的25$)与==椭圆抛物面==截取的<strong>曲线</strong></li>
</ul>
<p><img src="images/3.png" alt="等高线图" title="等高线图"></p>
<ul>
<li>每一个椭圆形显示了一系列$J(\theta_0,\theta_1)$值相等的点</li>
<li>这些同心椭圆的中心就是代价函数的最小值</li>
<li>梯度下降的方向是==沿着与等值线垂直的方向==</li>
</ul>
<h4 id="2-3-梯度下降"><a href="#2-3-梯度下降" class="headerlink" title="2.3 梯度下降"></a>2.3 梯度下降</h4><p>用梯度下降算法求任意函数$J(\theta_0,\theta_1,\theta_2,…\theta_n)$最小化函数值：</p>
<ul>
<li>多元函数的梯度，沿着梯度方向导数最大。可以把梯度下降的过程想象成下山坡, 如果想要尽可能快的下坡, 应该每次都往坡度最大的方向下山</li>
<li>梯度下降法是求局部最优解，受到初始值的影响，即当从不同的点开始时, 可能到达不同的局部极小值</li>
</ul>
<p>步骤：</p>
<ul>
<li>给定$\theta_0,\theta_1,…,\theta_n$的初始值（例如：$\theta_0 = 0,\theta_1 = 0,…,\theta_n = 0$）;</li>
<li>计算梯度。梯度也就是代价函数对每个$\theta$的偏导。将$h_{\theta}(x) = \theta_0 + \theta_{1}x^{(i)}$带入到$J(\theta_0,\theta_1)$中，并且分别对$\theta_0和\theta_1$求导；</li>
<li>不停的改变$\theta_0,\theta_1,…,\theta_n$来使$J(\theta_0,\theta_1,\theta_2,…\theta_n)$变小（注意要保持下图中的同步更新）;</li>
<li>直到找到$J(\theta_0,\theta_1,\theta_2,…\theta_n)$的最小值或者局部最小值。</li>
</ul>
<p><img src="images/4.png" alt="梯度下降法" title="梯度下降法"></p>
<p>repeat until convergence {<br>        $$\theta_j := \theta_j - \alpha\frac{\partial}{\partial_{\theta_j}}J(\theta_0,\theta_1) (for j=0 and j =1)$$<br>    }</p>
<p>注：</p>
<ul>
<li>$a := b$表示赋值,$a = b$表示断言$a$的值等于$b$的值，真假判断；</li>
<li>$\alpha$学习率(learning rate)用来控制以多大的幅度更新参数$\theta_j$；</li>
<li>$$\frac{\partial}{\partial_{\theta_j}}J(\theta_0,\theta_1) $$叫做梯度，代价函数$J(\theta)$沿着梯度方向变化最快；</li>
<li>==理论依据：==当偏导数等于零时，是代价函数$J(\theta)$取得极值的必要条件。当偏导数等于零后，$\theta$不再更新因为$\theta_j := \theta_j - \alpha\ast0$</li>
<li>同步更新：<img src="images/5.png" alt="同步更新"></li>
</ul>
<p>特殊情况：</p>
<ul>
<li>学习率$\alpha$会影响梯度下降的幅度<ul>
<li>1)太小梯度下降就会非常慢，需要很多步才找到局部最优点;</li>
<li>2)太大则$\theta$的值每次的变化会很大，有可能直接越过最低点，可能导致永远没法到达最低点。(可能越过最优点甚至可能无法收敛或者发散)。</li>
</ul>
</li>
<li>$\theta_0或\theta_1$处于局部最优点<ul>
<li>偏导数项为0，所以未做任何更新</li>
</ul>
</li>
<li>接近局部最优点时<ul>
<li>梯度下降法会自动采取更小的幅度，因为导数值会自动变得越来越小，而学习率不变，参数$\theta_n$的变化幅度也就越来越小，最后保持不变。</li>
</ul>
</li>
</ul>
<p><strong>计算梯度</strong>：</p>
<ul>
<li>根据定义, 梯度也就是代价函数对每个$\theta$的偏导<ul>
<li></li>
</ul>
</li>
</ul>
<p>batch梯度下降(Batch Gradient Descent)：使用全部样本数据的一种梯度下降方法。</p>
<p>求解线性回归，可以用梯度下降，也可以用==方程组法==，各有优点，前者更适合大数据集.</p>
<hr>
<h3 id="第三章-线性代数回顾"><a href="#第三章-线性代数回顾" class="headerlink" title="第三章 线性代数回顾"></a>第三章 线性代数回顾</h3><h4 id="3-1-矩阵和向量"><a href="#3-1-矩阵和向量" class="headerlink" title="3.1 矩阵和向量"></a>3.1 矩阵和向量</h4><p>矩阵(Matrix)是指由数字组成的矩形阵列</p>
<p>矩阵的维数(Dimension of matrix)：行数 x 矩阵的列数 (number of rows x number of columns)</p>
<p>矩阵元素(Matrix Elements or entries of matrix)：$A_{ij} = $  “$i,j$ entry” 在第$i$行，第$j$列</p>
<p>向量(Vector)是只有一列的矩阵(n x 1),$y_i$ = $i^{th}$ element</p>
<p>大写$A$表示矩阵，小写$a$表示向量</p>
<h4 id="3-2-加法和标量乘法"><a href="#3-2-加法和标量乘法" class="headerlink" title="3.2 加法和标量乘法"></a>3.2 加法和标量乘法</h4><p>矩阵加法：两个矩阵的每一个元素都逐个相加</p>
<p>标量(实数等)乘法：矩阵每一个元素乘以标量</p>
<h4 id="3-3-矩阵乘法"><a href="#3-3-矩阵乘法" class="headerlink" title="3.3 矩阵乘法"></a>3.3 矩阵乘法</h4><p>矩阵 x 向量 = 向量</p>
<p>$$\begin{bmatrix}<br>1 &amp; x_1\<br>1 &amp; x_2\<br>… &amp; …\<br>1 &amp; x_m<br>\end{bmatrix}$$ x $$\begin{bmatrix}<br>\theta_0\<br>\theta_1<br>\end{bmatrix}$$ = $$\begin{bmatrix}<br>\theta_0 + \theta_1 \ast  x_1\<br>\theta_0 + \theta_1 \ast  x_2\<br>… \<br>\theta_0 + \theta_1 \ast  x_m<br>\end{bmatrix}$$即：$$\begin{bmatrix}<br>y_1\<br>y_2\<br>… \<br>y_m<br>\end{bmatrix}$$ ==其中$m$为样本个数，参数矩阵增加行数n(即线性回归模型有n个变量)，参数矩阵增加列数o(即有o个线性回归模型)==</p>
<p>$Prediction = DataMatrix \times Parameters$</p>
<p>$A \times B \neq B \times A$</p>
<p>$A \times B \times C =  A \times (B \times C)$</p>
<p>单位矩阵$I_{n \times n}$：$A \times I = I \times A = A$</p>
<h4 id="3-4-逆和转置"><a href="#3-4-逆和转置" class="headerlink" title="3.4 逆和转置"></a>3.4 逆和转置</h4><p>矩阵的逆(Matrix inverse)：$AA^{-1} = A^{-1}A = I$</p>
<p><strong>只有方阵才有逆矩阵</strong></p>
<p>奇异矩阵(singular or degenerate)：非满秩矩阵没有逆矩阵</p>
<hr>
<h3 id="第四章-多变量线性回归"><a href="#第四章-多变量线性回归" class="headerlink" title="第四章 多变量线性回归"></a>第四章 多变量线性回归</h3><h4 id="4-1-多元或多变量"><a href="#4-1-多元或多变量" class="headerlink" title="4.1 多元或多变量"></a>4.1 多元或多变量</h4><p>多变量线性回归：</p>
<ul>
<li>常见符号：<ul>
<li>$n$ = 特征数量</li>
<li>$x^{(i)}$ = 第$i$个训练样本</li>
<li>$x_j^{i}$ = 第$i$个训练样本中第$j$个特征量的<strong>值</strong></li>
</ul>
</li>
<li>假设函数：，其中定义$x_0^{(i)} = 1$;</li>
<li>参数向量$$\theta = \begin{bmatrix}<br>\theta_0\<br>\theta_1\<br>\theta_2\<br>.\<br>.\<br>.\<br>\theta_n<br>\end{bmatrix} \in \mathbb{R}^{n+1}$$ ，特征向量$$x = \begin{bmatrix}<br>x_0\<br>x_1\<br>x_2\<br>.\<br>.\<br>.\<br>x_n<br>\end{bmatrix} \in \mathbb{R}^{n+1}$$</li>
</ul>
<p>这样就得到了假设函数的向量表示:<br>                            $$h_{\theta}(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + … + \theta_nx_n = \theta^Tx$$</p>
<h4 id="4-2-多元梯度下降法"><a href="#4-2-多元梯度下降法" class="headerlink" title="4.2 多元梯度下降法"></a>4.2 多元梯度下降法</h4><h5 id="4-2-1-定义"><a href="#4-2-1-定义" class="headerlink" title="4.2.1 定义"></a>4.2.1 定义</h5><p>代价函数：</p>
<p>$$ J(\theta_0,\theta_1,…,\theta_n) = \frac{1}{2m} \sum_{i=1}^{m} {(h_\theta(x^{(i)})-y^{(i)})^{2}} \tag{4.1} $$</p>
<p>​    Repeat {<br>​            $$\theta_j := \theta_j -\alpha \frac{1}{m} \sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$$ (simultaneously update $\theta_j$ for $j =0,1,2,…,n$)<br>​    }</p>
<h5 id="4-2-2-特征缩放-Feature-Scaling"><a href="#4-2-2-特征缩放-Feature-Scaling" class="headerlink" title="4.2.2 特征缩放(Feature Scaling)"></a>4.2.2 特征缩放(Feature Scaling)</h5><p>如果每个特征的范围相差的很大（等值线是比较扁的椭圆）, <strong>梯度下降会很慢</strong>。 为了解决这个问题, 我们在梯度下降之前应该对数据做特征缩放(Feature Scaling)处理, 从而将所有的特征的<strong>数量级</strong>都在一个差不多的范围之内（等值线近似于同心圆环）, 以加快梯度下降的速度。</p>
<p>通常我们需要把特征的取值都缩放到[−1,+1](附近)这个范围（<strong>同数量级</strong>），有特征在[0,3]附近也是可以的，但在[-100,100]就不行了。</p>
<p>==均值归一化==(Mean normalization)：</p>
<p>​            $$x_{i}^{‘} = \frac{x_i - \mu_i}{max - min}$$</p>
<p>或者</p>
<p>​            $$x_{i}^{‘} = \frac{x_i - \mu_i}{\delta_i}$$ 其中$\mu_i$表示第$i$个特征的平均值，$\delta_i$表示第$i$个特征的标准差</p>
<p><strong>注</strong>：计算后的均值和标准差应该保存起来，供进行预测时对新数据处理。</p>
<h5 id="4-2-3-学习率"><a href="#4-2-3-学习率" class="headerlink" title="4.2.3 学习率"></a>4.2.3 学习率</h5><p>梯度下降算法是否收敛：</p>
<ol>
<li>我们可以通过画出$\underset{\theta}{min}J(\theta)$与迭代次数数的关系图来观察梯度下降的运行：<ul>
<li>判定梯度下降算法是否运行正常（下降至与横坐标平行），如果曲线是上升的或者振荡的，则算法运行不正常，需要较小的学习率$\alpha$</li>
<li>只要学习率$\alpha$足够小，每次迭代之后代价函数$J(\theta)$都会下降</li>
<li>查看梯度下降算法是否收敛（是否出现与横坐标平行）</li>
</ul>
</li>
<li>自动收敛测试：如果代价函数$J(\theta)$一步迭代后的下降小于一个很小的值$\epsilon$ ，这个测试就判断函数已收敛，例如$\epsilon$可以是$1e^{-3}$。</li>
</ol>
<p>选取$\alpha$的经验：</p>
<ol>
<li>按照3倍设置，即0.001-&gt;0.003-&gt;0.01</li>
<li>尝试一系列$\alpha$值，直到找到一个太小的值，再找到一个太大的$\alpha$值</li>
<li>取最大可能值或者比最大值略小一些的比较合理的值</li>
</ol>
<h5 id="4-2-4-特征和多项式回归-Polynomial-Regression"><a href="#4-2-4-特征和多项式回归-Polynomial-Regression" class="headerlink" title="4.2.4 特征和多项式回归(Polynomial Regression)"></a>4.2.4 特征和多项式回归(Polynomial Regression)</h5><p>自由选择使用什么特征并且通过设计不同的特征，能够用更加复杂的函数拟合数据。比如把某个特征的平方，立方作为两个新的特征，则可用一个3次多项式模型。</p>
<p>从合适的角度寻找特征(定义新的特征等)，就能得到一个更符合数据的模型。</p>
<p>如何将<strong>模型</strong>和数据(<strong>特征</strong>)进行拟合：</p>
<ul>
<li>通过设计不同的特征，能够用更复杂的模型拟合数据。比如把某个特征的平方，立方作为两个新的特征，则可用一个3次多项式模型</li>
<li>选择使用哪些特征</li>
</ul>
<p>==线性回归将$n$维的变量通过一次或者高次的组合，模拟出在$n$维坐标下的曲线(不一定是直线)，进行线性预测或者分类==</p>
<h5 id="4-2-5-正规方程Normal-Equations（区别与迭代方法的直接法）"><a href="#4-2-5-正规方程Normal-Equations（区别与迭代方法的直接法）" class="headerlink" title="4.2.5 正规方程Normal Equations（区别与迭代方法的直接法）"></a>4.2.5 正规方程Normal Equations（区别与迭代方法的直接法）</h5><p>最优值常规解法：<br>        $$\theta \in \mathbb{R}^{n+1}$$        $$ J(\theta_0,\theta_1,…,\theta_n) = \frac{1}{2m} \sum_{i=1}^{m} {(h_\theta(x^{(i)})-y^{(i)})^{2}}$$</p>
<p>​        $$\frac{\partial}{\partial \theta_j}J(\theta) = \cdots = 0$$   (for every $j$)</p>
<p>​    ==Solve for== $\theta_0,\theta_1, \cdots ,\theta_n$</p>
<p>正规方程：</p>
<p>$$\theta = (X^TX)^{-1}X^{T}y$$</p>
<pre><code>Octave:pinv(X&apos;*X)*X&apos;*y pinv是伪逆函数，逆不存在也能返回值
</code></pre><p>正规方程与梯度下降法的优缺点：</p>
<ul>
<li>梯度下降法：（特征数n&gt;10000)<ul>
<li>需要选择学习率$\alpha$</li>
<li>需要很多次迭代，可能需要进行特征缩放</li>
<li>当特征变量很多的情况下，也能运行地相当好</li>
</ul>
</li>
<li>正规方程：（特征数n&lt;10000)<ul>
<li>不需要选择学习率$\alpha$</li>
<li>不需要迭代，不需要进行特征缩放</li>
<li>需要计算$(X^TX)^{-1}$,其维度是$n \times n$，求逆运算的计算量是O($n^3$)</li>
<li>当特征变量很多的情况下，计算比较慢</li>
</ul>
</li>
</ul>
<h5 id="4-2-6-正规方在矩阵不可逆的情况下的解法"><a href="#4-2-6-正规方在矩阵不可逆的情况下的解法" class="headerlink" title="4.2.6 正规方在矩阵不可逆的情况下的解法"></a>4.2.6 正规方在矩阵不可逆的情况下的解法</h5><p>$(X^TX)^{-1}$不存在：</p>
<ul>
<li>包含了多余的特征(存在两个特在线性相关)</li>
<li>太多的特征参数，相对的，样本数过少$m&lt;n$<ul>
<li>删除一些特征或者使用正则化处理</li>
</ul>
</li>
</ul>
<hr>
<h3 id="第五章-Octave-Matlab-教程"><a href="#第五章-Octave-Matlab-教程" class="headerlink" title="第五章 Octave/Matlab 教程"></a>第五章 Octave/Matlab 教程</h3><h4 id="5-1-逻辑运算"><a href="#5-1-逻辑运算" class="headerlink" title="5.1 逻辑运算"></a>5.1 逻辑运算</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">等于：== 不等于：~= 与：&amp;&amp; 或：|| 异或：xor(1,0)</span><br></pre></td></tr></table></figure>
<h4 id="5-2-Octave常用命令"><a href="#5-2-Octave常用命令" class="headerlink" title="5.2 Octave常用命令"></a>5.2 Octave常用命令</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. help help %查看help的帮助</span><br><span class="line">2. format long %平时输出的都是以长整型为精度</span><br><span class="line">3. disp(sprintf(&apos;2 decimals: %0.2f&apos;, pi)) %输出圆周率π并取精度为小数点后两位</span><br><span class="line"></span><br><span class="line">4. who %显示当前Octave内存中储存的所有变量</span><br><span class="line">5. whos %显示当前Octave内存中储存的所有变量及变量详细</span><br><span class="line">6. clear A %将变量A从当前Octave内存中清除</span><br></pre></td></tr></table></figure>
<p><strong>其他：</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1. 添加分号可以抑制输出</span><br><span class="line">2. 圆周率π： pi</span><br><span class="line">3. 常数e： e</span><br></pre></td></tr></table></figure>
<h4 id="5-3-矩阵"><a href="#5-3-矩阵" class="headerlink" title="5.3 矩阵"></a>5.3 矩阵</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">1. 构造一个矩阵：</span><br><span class="line">			  1) A = [1 2; 3 4; 5 6;]  %分号开始下一行</span><br><span class="line">			  2) A = [1 2; %回车</span><br><span class="line">			  		3 4;   %回车</span><br><span class="line">			  		5 6;   %回车</span><br><span class="line">			  		]      %回车</span><br><span class="line">			  3）v = 1:0.2:2 %以0.2为步距从1到2构造行向量</span><br><span class="line">              </span><br><span class="line">2. ones(2,3) %构造一个所有元素均为1的2 x 3 矩阵</span><br><span class="line">3. rand(3,3) %由9个高斯随机数作为元素构造3 x 3 矩阵</span><br><span class="line">4. zeros(1,3) %构造一个所有元素均为0的1 x 3 矩阵</span><br><span class="line">5. eye(5) %维度为5的单位矩阵</span><br><span class="line">   linspace(1,10,5) %创建一个5个元素的向量, 均匀分布于1和10</span><br><span class="line">   logspace(1,10,5) %创建一个5个元素的向量，指数分布与1和10之间</span><br><span class="line"></span><br><span class="line">6. A(3,2) %读取矩阵A第三行第二列的元素</span><br><span class="line">7. A(:,2) %读取矩阵A第2列所有元素</span><br><span class="line">8. A(2,:) %读取矩阵A第2行所有元素</span><br><span class="line">9. A([1 3],:) %读取矩阵A第1行和第3行的所有元素</span><br><span class="line">10. A(1:3) %按列的顺序，取第一个到第三个的数值</span><br><span class="line"></span><br><span class="line">11. A(:,2) = [10; 11; 12] %将A第二列替换为[10;11;12]</span><br><span class="line">12. A = [A, [100; 101; 102]] %在A的最后加上一列</span><br><span class="line">13. A(:) %将A所有的元素合并成一个列向量</span><br><span class="line">14. C = [A B] %两个矩阵的合并（列合并）</span><br><span class="line">15. D = [A;B] %两个矩阵的合并（行合并）</span><br><span class="line"></span><br><span class="line">16. size(A) %以1 x 2的矩阵形式返回A矩阵的大小</span><br><span class="line">17. size(A,1) %A矩阵第一维度的大小（行数）</span><br><span class="line">18. size(A,2) %A矩阵第二维度的大小（列数）</span><br><span class="line">19. length(v) %v最大维度的大小</span><br></pre></td></tr></table></figure>
<h4 id="5-4-数据操作"><a href="#5-4-数据操作" class="headerlink" title="5.4 数据操作"></a>5.4 数据操作</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">1. cd Desktop/ %找到文件目录</span><br><span class="line">2. 读取数据：</span><br><span class="line">			1) load xxx.dat</span><br><span class="line">			2) load (&apos;xxx.dat&apos;)</span><br><span class="line">3. 储存数据：</span><br><span class="line">			1）save hello.mat v %使变量v以二进制形式保存为一个名为hello.mat的文件</span><br><span class="line">			2）save hello.txt v  -ascii %使变量v以文本形式保存为一个名为hello.txt的文件</span><br><span class="line">4. 操作数据：</span><br><span class="line">			1)矩阵运算：</span><br><span class="line">						(1) A * B   %A乘B</span><br><span class="line">						</span><br><span class="line">							% .dot 针对元素的运算</span><br><span class="line">						(2) A .* B  %A中的元素与B中对应的元素相承</span><br><span class="line">						(3) A .^ 2  %A中的元素进行平方</span><br><span class="line">						(4) 1 ./ v  %求v对应元素的倒数</span><br><span class="line">							</span><br><span class="line">						(5) log(v)  %对v中所有元素求对速运算</span><br><span class="line">						(6) exp(v)  %以e为底，以v中元素为指数的幂运算</span><br><span class="line">						(7) abs(v)  %求v中所有元素的绝对值</span><br><span class="line">						(8) max(a)  %向量a中最大的元素</span><br><span class="line">						(8) max(A)  %会得到矩阵A中每一列的最大值</span><br><span class="line">						(9) [val,ind] = max(a)  %val是a中最大的值，ind是a中该元素的索引</span><br><span class="line">						(10) max(A,[],1)   %A的每一列最大值组成的行向量</span><br><span class="line">						(11) max(A,[],2)   %A的每一行最大值组成的列向量</span><br><span class="line">						(12) A = magic(3)  %返回名为幻方的3 x 3方阵(任意行 列 对角线中的元素加起来都等于相同的值)</span><br><span class="line">						(13) sum(a)   %a中所有元素的和</span><br><span class="line">						(14) prod(a)  %product a中所有元素的乘积</span><br><span class="line">						(15) floor(a) %对a中所有元素向下取整</span><br><span class="line">						(16) ceil(a)  %对a中所有元素向上取整</span><br><span class="line">						(17) flipud(A) %对方阵A进行垂直翻转</span><br><span class="line">						</span><br><span class="line">						(18) -v      %求v中所有元素的相反数等价于-1乘以v</span><br><span class="line">						(19) v + 1   %v中所有元素加一</span><br><span class="line">						(20) a &lt; 3  %根据对应元素比较结果的真和假作为与a相同类型矩阵的元素</span><br><span class="line">						(21) find(a &lt; 3)  %找出a中所有小于3的元素并返回他们的索引</span><br><span class="line">						(22) [r,c] = find(a &gt;= 7)  %找出a中所有大于等于7的元素并返回他们的索引</span><br><span class="line">						</span><br><span class="line">						(23) A&apos;      %求A的转置</span><br><span class="line">						(24) pinv(A) %求A的伪逆矩阵</span><br><span class="line">						</span><br><span class="line">						(25) mean(A) %求A的平均值</span><br><span class="line">						(26) std(A)  %求A的标准差</span><br><span class="line">						(27) @ %变量名=@(输入参数列表)运算表达式;句柄@同样可以指向自定义函数</span><br><span class="line">						(28) bsxfun(@minus, A, v) %自动将运算的矩阵进行匹配复制</span><br></pre></td></tr></table></figure>
<h4 id="5-5-数据绘制"><a href="#5-5-数据绘制" class="headerlink" title="5.5 数据绘制"></a>5.5 数据绘制</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">plot(x,y)</span><br><span class="line">hold on %将两个函数绘制在一起</span><br><span class="line">xlabel %标注横坐标</span><br><span class="line">ylabel %标注纵坐标</span><br><span class="line">legend %标注图线</span><br><span class="line">print -dpng &apos;filename.png&apos; %存储图像</span><br><span class="line">close %关掉绘制的图像</span><br><span class="line">figure(1) %在不同窗口绘制图像</span><br><span class="line">subplot(1,2,1) %在同一窗口的第一个位置绘制图像</span><br><span class="line">axis([x1 x2 y1 y2]) %改变图像的横坐标的刻度和纵坐标的刻度</span><br><span class="line">clf; %清除所有绘制的图像</span><br><span class="line">imagesc %将矩阵可视化</span><br><span class="line">colorbar</span><br><span class="line">colormap</span><br><span class="line"></span><br><span class="line">contour %绘制等高线</span><br><span class="line">surf %网状图绘制</span><br></pre></td></tr></table></figure>
<h4 id="5-6-控制语句"><a href="#5-6-控制语句" class="headerlink" title="5.6 控制语句"></a>5.6 控制语句</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">1. for语句：</span><br><span class="line">			for i=1:10,</span><br><span class="line">     			v(i)=2^i;</span><br><span class="line">			end;</span><br><span class="line">2. while语句：</span><br><span class="line">			i=1;</span><br><span class="line">			while i&lt;=5,</span><br><span class="line">  				v(i)=100;</span><br><span class="line">				i=i+1;</span><br><span class="line">			end;</span><br><span class="line">3. break语句：</span><br><span class="line">			i=1;</span><br><span class="line">			while true,</span><br><span class="line">  				v(i)=999;</span><br><span class="line">  				i=i+1;</span><br><span class="line">  				if i==6,</span><br><span class="line">    				break;</span><br><span class="line">  				end;</span><br><span class="line">			end;</span><br><span class="line">4. if语句：</span><br><span class="line">			v(1)=2;</span><br><span class="line">			if v(1)==1,</span><br><span class="line">  				disp(&apos;The value is one&apos;);</span><br><span class="line">			elseif v(1)==2,</span><br><span class="line">  				disp(&apos;The value is two&apos;);</span><br><span class="line">            else</span><br><span class="line">              disp(&apos;The value is not one or two.&apos;);</span><br><span class="line">            end;</span><br></pre></td></tr></table></figure>
<h4 id="5-6-函数调用"><a href="#5-6-函数调用" class="headerlink" title="5.6 函数调用"></a>5.6 函数调用</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">1. 先要保证当前路劲中有本函数，所以可能需要更改路径或者添加搜索路径</span><br><span class="line">cd &apos;C:\Users\ang\Desktop&apos;</span><br><span class="line">squareThisNumber(5)</span><br><span class="line">或者</span><br><span class="line">addpath(&apos;C:\Users\ang\Desktop&apos;)</span><br><span class="line">cd &apos;C:\&apos;</span><br><span class="line">squareThisNumber(5)</span><br><span class="line"></span><br><span class="line">2. 函数中定义平方与立方</span><br><span class="line">		将会返回两个值 多个返回值</span><br><span class="line">		[a, b]=squareAndCubeThisNumber(5);</span><br><span class="line">		</span><br><span class="line">3. 定义函数，计算不同θ值的代价函数J(θ)</span><br><span class="line">		function J=costFunctionJ(X, y, theta)</span><br><span class="line">        m=size(X, 1);</span><br><span class="line">        predictions=X*theta;</span><br><span class="line">        sqrErrors=(predictions-y).^2</span><br><span class="line">        J=1/(2*m)*sum(sqrErrors);</span><br><span class="line">        </span><br><span class="line">        以上是函数定义文档，下面做调用。</span><br><span class="line">        X=[1 1; 1 2; 1 3]</span><br><span class="line">        y=[1; 2; 3]</span><br><span class="line">        theta=[0; 1];</span><br><span class="line">        j=costFunctionJ(X, y, theta)</span><br></pre></td></tr></table></figure>
<h4 id="5-7-矢量"><a href="#5-7-矢量" class="headerlink" title="5.7 矢量"></a>5.7 矢量</h4><hr>
<h3 id="第六章-Logistic回归"><a href="#第六章-Logistic回归" class="headerlink" title="第六章 Logistic回归"></a>第六章 Logistic回归</h3><h4 id="6-1-分类-不能直接使用线性回归的原因"><a href="#6-1-分类-不能直接使用线性回归的原因" class="headerlink" title="6.1 分类(不能直接使用线性回归的原因)"></a>6.1 分类(不能直接使用线性回归的原因)</h4><p><img src="images/6.png" alt="线性回归" title="线性回归分类"></p>
<p>我们设定一个阈值0.5, 当假设函数的输出大于这个阈值时我们预测$y=1$；当假设函数的输出小于0.5时, 我们预测$y=0$。<strong>1）</strong>但是如果我们再增加一个数据(上图最右), 使用Linear Regression就会得到如图蓝色线所示的$h_{\theta}(x)$。这个结果显然是不合理的, 它有很多错误的分类. 直观上来看, 要是能得到图中垂直于横轴的(图中蓝色)线那边是极好的。<strong>2）</strong>$h_{\theta}(x)$输出可能会远大于1或小于0</p>
<h4 id="6-2-假设函数"><a href="#6-2-假设函数" class="headerlink" title="6.2 假设函数"></a>6.2 假设函数</h4><p>$$h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}} \tag{6.1}$$</p>
<p>上式输出的是$h_{\theta}(x) = P(y=1|x;\theta)$估值概率 表示在$x$的条件下，$y=1$的概率，这个概率的参数是$\theta$；例如我们给定一个$x$, 它的假设函数的输出为0.7, 我们可以说这个病人的肿瘤为恶性的概率是70%.</p>
<h4 id="6-3-决策边界-decision-boundary"><a href="#6-3-决策边界-decision-boundary" class="headerlink" title="6.3 决策边界(decision boundary)"></a>6.3 决策边界(decision boundary)</h4><p>假设预测”$y=1$”如果$h_{\theta}(x) \geqslant 0.5$ 即$\theta^Tx \geqslant 0$</p>
<p>​    预测“$y=0$”如果$h_{\theta}(x) &lt; 0.5$ 即$\theta^Tx &lt; 0$</p>
<p>==决策边界是假设函数的一个属性，取决于其参数$\theta$，不是数据集的属性，==数据集是用来拟合参数的。</p>
<p><img src="images/7.png" alt="决策边界" title="决策边界"></p>
<p>上图的决策边界应该是以原点为圆心半径为1的圆。</p>
<h4 id="6-4-代价函数"><a href="#6-4-代价函数" class="headerlink" title="6.4 代价函数"></a>6.4 代价函数</h4><p>在线性回归中, 之所以可以使用梯度下降来找到全局最优解是因为代价函数$J(\theta)$是一个凸函数(convex)。 但是对于对率回归来说, 假设函数$h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}$是一个较为复杂的<strong>非线性函数</strong>, 直接带入的话得到的代价函数就不是一个凸函数(non-convex), 如下图左侧部分所示. 这样使用梯度下降只能找到局部最优。</p>
<p><img src="images/8.png" alt="凸函数"></p>
<p>逻辑回归的代价函数：</p>
<p>$$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_{\theta}(x^{(i)}),y^{(i)})$$</p>
<p>$$Cost(h_{\theta}(x),y) = \left{\begin{matrix}<br>-log(h_{\theta}(x)) \textrm{   if } y = 1\<br>-log(1-h_{\theta}(x)) \textrm{ if } y=0<br>\end{matrix}\right.$$</p>
<p><img src="images/9.png" alt="y=1代价函数" title="y=1代价函数"></p>
<p>当$y=1$时, $Cost(h_{\theta}(x),y)$如上图所示. 此时, 如果我们$h_{\theta}(x)=1$, 那么$Cost=0$, 即当预测结果和真实结果一样时, 我们不对学习算法进行惩罚；但是当结果不一致时, 即$h_{\theta}(x)→0$时, 我们对算法的惩罚趋近于无穷。</p>
<p><img src="images/10.png" alt="y=0代价函数" title="y=0代价函数"></p>
<p>上图是当$y=0$时的情况。</p>
<h4 id="6-5-简化代价函数与梯度下降"><a href="#6-5-简化代价函数与梯度下降" class="headerlink" title="6.5 简化代价函数与梯度下降"></a>6.5 简化代价函数与梯度下降</h4><p>简化的代价函数：</p>
<p>$$Cost(h_{\theta}(x),y) = -y log(h_{\theta}(x)) - (1-y)log(1-h_{\theta}(x))$$</p>
<p>$$J(\theta) = \frac{1}{m}\sum_{i=1}^{m}Cost(h_{\theta}(x^{(i)}),y^{(i)})$$</p>
<p>$$=-\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}logh_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))]$$</p>
<p>注：==来自极大似然函数==：$L(\theta)=L(X_1,X_2,\cdots,X_n;\theta)=\prod_{i=1}^{n}f(X_i;\theta)$，其中$f(X_i;\theta)$是我们自己选择的函数模型。我们使用的是$h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}$；</p>
<p>梯度下降：</p>
<p>需要$min_{\theta}J(\theta)$:</p>
<p>Repeat {</p>
<p>$$\theta_j := \theta_j-\alpha\frac{\partial }{\partial \theta_j}J(\theta)$$</p>
<p>即：$$\theta_j := \theta_j-\alpha\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)}$$<br>    }</p>
<p>注：</p>
<ul>
<li>表现形式和线性回归一样，但由于模型$h_{\theta}(x)$改变里，其实不一样。</li>
<li>求导过程？</li>
</ul>
<p>如果特征之间的数量级差别较大也是需要特征缩放的。</p>
<h4 id="6-6-高级优化"><a href="#6-6-高级优化" class="headerlink" title="6.6 高级优化"></a>6.6 高级优化</h4><p>给出$\theta$，我们需要能够计算：</p>
<ul>
<li>计算$J(\theta)$以便代码监控代价函数$J(\theta)$的收敛特性；</li>
<li>计算偏导数项$\frac{\partial }{\partial \theta_j}J(\theta)$以便实现梯度下降或者供高级优化算法使用。</li>
</ul>
<p><img src="images/11.png" alt="优化算法" title="优化算法"></p>
<p>优化算法一般不需要自己编写，只需要使用代码库。同一编程语言的不同实现优化算法效果不同，在使用时需要多试几个不同的代码库。</p>
<p>高级优化算法是一个以代价函数$J(\theta)$和代价函数的偏导数项$\frac{\partial }{\partial \theta_j}J(\theta)$为输入，输出能够使代价函数最小的$\theta$值。功能和梯度下降算法一样，但是实现不一样，两者优缺点见上图。</p>
<h4 id="6-7-多元分类：一对多"><a href="#6-7-多元分类：一对多" class="headerlink" title="6.7 多元分类：一对多"></a>6.7 多元分类：一对多</h4><p>将多分类问题转化为二分类问题：</p>
<ul>
<li>对每个类别$i$训练一个逻辑回归分类器$h_{\theta}^{(i)}(x)$(将第$i$类看作正类，其它类看作负类的分类器)，此分类器预测了$y=i$的可能性；</li>
<li>对一个新的输入$x$作出预测，只需选取以上分类器对于新输入$x$的最大值$\underset{i}{\max}h_{\theta}^{(i)}(x)$</li>
</ul>
<hr>
<h3 id="第七章-正则化-Regularization"><a href="#第七章-正则化-Regularization" class="headerlink" title="第七章 正则化(Regularization)"></a>第七章 正则化(Regularization)</h3><h4 id="7-1-过拟合问题"><a href="#7-1-过拟合问题" class="headerlink" title="7.1 过拟合问题"></a>7.1 过拟合问题</h4><p>欠拟合(underfitting)或高偏差(high bias)：模型没有很好地捕捉到数据特征，不能够很好地拟合数据。</p>
<p>过拟合(overfitting)或高方差(high variance)：模型把数据学习的太彻底，以至于把噪声数据的特征也学习到了，这样就会导致在后期测试的时候不能够很好地识别数据，即不能正确的分类，模型泛化能力太差。</p>
<p><img src="images/12.png" alt="拟合问题" title="拟合问题"></p>
<p>过拟合可能原因：</p>
<ul>
<li>太多的特征，导致学得的模型与训练集非常适合(代价函数$J(\theta)\approx0$)，但是失去了泛化能力。</li>
</ul>
<p>过拟合解决办法：</p>
<ul>
<li><p>减少选取特征的数量</p>
<ul>
<li>手动选择保留哪些特征；</li>
<li>模型选择算法。</li>
</ul>
</li>
<li><p>正则化</p>
<ul>
<li>保留所有特征变量，减少$\theta_j$的大小和量级；</li>
<li>每一个特征都能对预测的$y$值产生一点影响。</li>
</ul>
</li>
</ul>
<h4 id="7-2-含正则化项的代价函数"><a href="#7-2-含正则化项的代价函数" class="headerlink" title="7.2 含正则化项的代价函数"></a>7.2 含正则化项的代价函数</h4><p>  较小的参数值$\theta_0,\theta_1,\cdots,\theta_n$：</p>
<ul>
<li>“更简单“的假设函数，函数图像就会越平滑；</li>
<li>更不容易出现过拟合现象。</li>
</ul>
<p>线性回归含正则化项的代价函数：</p>
<p>$$J(\theta) = \frac{1}{2m}[\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^{n}\theta_j^2]$$</p>
<ul>
<li>正则化项缩小了每个参数的值；</li>
<li>正则化项的求和是从$\theta_1$开始的(因为$\theta_0$不需要$\approx0$)，也可以把$\theta_0$加上去；</li>
<li>$\lambda$正则化参数，控制以下两个不同目标之间的平衡关系：<ul>
<li>代价函数的第一项，更好的拟合训练集；</li>
<li>正则化项保持参数尽量小。</li>
</ul>
</li>
</ul>
<p>$\lambda$影响每个参数值的大小：</p>
<ul>
<li>$\lambda$太大，得到的参数值就会太小，最终模型近似于一个常数，导致欠拟合；</li>
</ul>
<h4 id="7-4-线性回归正则化"><a href="#7-4-线性回归正则化" class="headerlink" title="7.4 线性回归正则化"></a>7.4 线性回归正则化</h4><p>梯度下降法：</p>
<p>​    Repeat {<br>​            $$\theta_0 := \theta_0 -\alpha \frac{1}{m} \sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)})x_0^{(i)}$$<br>​            $$\theta_j := \theta_j(1-\alpha\frac{\lambda}{m}) -\alpha \frac{1}{m} \sum_{i=1}^m(h_{\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$$ (simultaneously update $\theta_j$ for $j =1,2,…,n$)<br>​    }</p>
<ul>
<li>$1-\alpha\frac{\lambda}{m}$是只比1略小一点的数(可以看作0.99)，通常学习率很小，但$m$却很大；</li>
<li>进行正则化线性回归时，从直观上看每次迭代更新都把参数缩小一点，然后和非正则化线性回归一样的更新操作；而实际是梯度下降求导的结果。</li>
</ul>
<p>正则方程：</p>
<p>$$\theta = (X^TX + \lambda \begin{bmatrix}<br>0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0\<br>0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0\<br>0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0\<br>\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\<br>0 &amp; 0 &amp; 0 &amp; 0 &amp; 1<br>\end{bmatrix})^{-1}X^Ty$$</p>
<p>如果$\lambda&gt;0$，上式的逆一定存在。</p>
<h4 id="7-5-logistic回归正则化"><a href="#7-5-logistic回归正则化" class="headerlink" title="7.5 logistic回归正则化"></a>7.5 logistic回归正则化</h4><p>逻辑回归含正则化的代价函数：</p>
<p>$$J(\theta) = -\frac{1}{m}[\sum_{i=1}^{m}y^{(i)}logh_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^{n}\theta_j^2$$</p>
<p>梯度下降和线性回归表达式一样，由于两者假设函数不同，故其梯度下降的求导过程不同(虽然结果相同)。</p>
<hr>
<h3 id="第八章-神经网络学习"><a href="#第八章-神经网络学习" class="headerlink" title="第八章 神经网络学习"></a>第八章 神经网络学习</h3><h4 id="8-1-非线性假设"><a href="#8-1-非线性假设" class="headerlink" title="8.1 非线性假设"></a>8.1 非线性假设</h4><p>当训练集的特征数量较多时，用逻辑回归的方法(通过增加特征：比如增加高阶多项式项数)会导致有过多的参数($参数数量 = \frac{n^{模型的最高次数}}{模型的最高次数的阶乘}$），前面的做法会使得特征空间急剧膨胀，计算复杂度和耗时显著增大，最后的结果也很有可能是过拟合的。当然可以使用所有二次项的子集, 例如只用平方项$x_1^2,x_2^2,x_3^2,\cdots,x_n^2$，但是这样可能又欠拟合。</p>
<h4 id="8-2-神经元与大脑"><a href="#8-2-神经元与大脑" class="headerlink" title="8.2 神经元与大脑"></a>8.2 神经元与大脑</h4><p>大脑的神经接上不同的传感器就可以学习新的东西。例如：将照像机的图像通过电平阵列的方式输出到盲人的舌头上，盲人可以学会看东西。</p>
<h4 id="8-3-神经网络模型"><a href="#8-3-神经网络模型" class="headerlink" title="8.3 神经网络模型"></a>8.3 神经网络模型</h4><p><img src="images/13.png" alt="神经元模型" title="神经元模型"></p>
<p>上图为一个神经元(neuron)，它的输入为$x_1,x_2,x_3$，有时候为了方便我们添加一个$x_0$叫做bias unit。它的输出为$h(\theta)。$$\theta$也叫做weights.</p>
<p><img src="images/14.png" alt="神经网络" title="神经网络"></p>
<p>上图为由多个神经元组成的神经网络. 第一层叫做输入层(input layer), 最后一层叫做输出层(output layer), 中间的叫做隐藏层(hidden layer).</p>
<p>如果一个神经元的激活函数为sigmoid函数，则可以将它看作逻辑回归模型，只是它的输入是由其它神经元的输出作为特征的。==神经网络就相当于通过初始特征学习到新的特征, 新的特征再通过激活函数得到最终输出结果==。</p>
<p>术语：</p>
<ul>
<li>$a_i^j$表示第$j$层第$i$个神经元的激活项；</li>
<li>$\theta^j$权重矩阵表示控制从第$j$层到第$j+1$层的映射；</li>
<li>$\theta_{ij}^l$的$i$表示第$l+1$层神经单元，$j$表示第$l$层的神经单元。</li>
</ul>
<p><img src="images/15.png" alt="神经网络模型" title="神经网络模型"></p>
<p>向量化：</p>
<p>$$x = \begin{bmatrix}<br>x_0\<br>x_1\<br>x_2\<br>x_3<br>\end{bmatrix} z^{(2)} = \begin{bmatrix}<br>z_1^{(2)}\<br>z_2^{(2)}\\<br>z_3^{(2)}\<br>\end{bmatrix}$$</p>
<p>$$z^{(2)} = \theta^{(1)}x = \theta^{(1)}a^{(1)}$$</p>
<p>$$a^{(2)} = g(z^{(2)})$$</p>
<p>我们在隐藏层加上一个额外的 $$a_0^{(2)}=1$$，得到$$a^{(2)} = \begin{bmatrix}<br>a_0^{(2)}\<br>a_1^{(2)}\<br>a_2^{(2)}\<br>a_3^{(2)}<br>\end{bmatrix}$$</p>
<p>同理$$z^{(3)}=\theta^{(2)}a^{(2)}$$</p>
<p>最后$$h_{\theta}(x) = a^{(3)} = g(z^{(3)})$$</p>
<p>如果神经网络在$j$层有$s_j$个神经单元，在$j+1$层有$s_{j+1}$个神经单元，则$\theta^{(j)}$的维度为$s_{j+1} \times (s_j+1)$。</p>
<p>前向传播(Feedforward propagation)：从输入层到隐藏层再到输出层的过程。</p>
<p>神经网络中神经元的连接方式称为神经网络的架构。</p>
<p>神经网络学习复杂的非线性假设函数，并可以拟合任意函数。</p>
<hr>
<h3 id="第九章-神经网络参数的反向传播算法"><a href="#第九章-神经网络参数的反向传播算法" class="headerlink" title="第九章 神经网络参数的反向传播算法"></a>第九章 神经网络参数的反向传播算法</h3><h4 id="9-1-代价函数"><a href="#9-1-代价函数" class="headerlink" title="9.1 代价函数"></a>9.1 代价函数</h4><p>术语：</p>
<ul>
<li>$L$表示神经网络的层数；</li>
<li>$s_l$表示$l$层神经单元数(不包含$l$层偏差单元)。</li>
</ul>
<p>神经网络代价函数：</p>
<p>$$J(\theta) = -\frac{1}{m}[\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}log(h_{\theta}(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_{\theta}(x^{(i)}))<em>k)] + \frac{\lambda}{2m}\sum</em>{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\theta_{ji}^{(l)})^2$$</p>
<p>$$h_{\theta}(x)\in\mathbb{R}^K$$ 神经网络的输出是$K$维向量 $$(h_{\theta}(x^{(i)}))_k = 第i个样本k^{th}$$output</p>
<p>如果神经元的激活函数是sigmoid函数，则一个神经元是一个逻辑回归模型；现在神经网络的输出有$K$个神经元，故其代价函数是$K$个逻辑回归代价函数之和。</p>
<h4 id="9-2-反向传播算法-Backpropagation-algorithm"><a href="#9-2-反向传播算法-Backpropagation-algorithm" class="headerlink" title="9.2 反向传播算法(Backpropagation algorithm)"></a>9.2 反向传播算法(Backpropagation algorithm)</h4><p>$\delta_j^{(l)}$代表了第$l$层第$j$个节点的误差。</p>
<p>$\delta^{(l)}$的计算：</p>
<ul>
<li>输出层 $\delta^{(l)} = a^{(l)} - y$；</li>
<li>隐藏层 $\delta^{(l)} = (\theta^{(l)})^T \delta^{(l+1)}  .\ast g^{‘}(z^{(l)})$，如果$g(z^{(l)})$是sigmoid函数，则$g^{‘}(z^{(l)}) = a^{(l)} .\ast (1-a^{(l)})$</li>
<li>输入层 训练集观察到的值，不存在误差</li>
</ul>
<p>$\frac{\partial }{\partial \theta_{ij}^{(l)}}J(\theta)$的计算：</p>
<ul>
<li>忽略$\lambda$ $\frac{\partial }{\partial \theta_{ij}^{(l)}}J(\theta) = a_j^{(l)}\delta_i^{(l+1)}$;</li>
<li>不忽略$\lambda$：<ul>
<li>$\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}\delta_i^{(l+1)}$</li>
<li>$D_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} + \lambda\theta_{ij}^{(l)}$ if $j\neq0$<br>$D_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)}$ if $j=0$</li>
<li>$\frac{\partial }{\partial \theta_{ij}^{(l)}}J(\theta) = D_{ij}^{(l)}$</li>
</ul>
</li>
</ul>
<p>反向传播法，事实是用来==求解神经网络各层参数对代价函数的偏导数==，求得的偏导数可用于梯度下降等算法，进而得到整个神经网络的参数。</p>
<h4 id="9-3-理解反向传播算法"><a href="#9-3-理解反向传播算法" class="headerlink" title="9.3 理解反向传播算法"></a>9.3 理解反向传播算法</h4><p><img src="images\16.png" alt="theta的推导" title="theta的推导公式"></p>
<p>利用反向传播算法，计算代价函数的导数。</p>
<h4 id="9-4-使用注意：展开参数"><a href="#9-4-使用注意：展开参数" class="headerlink" title="9.4 使用注意：展开参数"></a>9.4 使用注意：展开参数</h4><p><img src="images/17.png" alt title="向量和矩阵转化"></p>
<p>利用梯度下降法，求取参数时需要将输入参数的格式转化(向量$\Leftrightarrow$矩阵)</p>
<p><img src="images/18.png" alt title="向量和矩阵转化"></p>
<h4 id="9-5-梯度检测"><a href="#9-5-梯度检测" class="headerlink" title="9.5 梯度检测"></a>9.5 梯度检测</h4><p>检测前向传播和反向传播参数是否计算正确，避免程序bug</p>
<p>双侧差分：$$\frac{\mathrm{d} }{\mathrm{d} \theta}J(\theta) \approx \frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}$$，$\epsilon$一般取$10^{-4}$</p>
<p><img src="images/19.png" alt="偏导情况的双侧差分" title="偏导情况的双侧差分"></p>
<p>注：</p>
<ul>
<li>当检验反向传播的实现是正确的后，一定要<strong>关闭梯度检验</strong>；</li>
<li>检查反向传播算法时，可以建立一个简单的模型进行。</li>
</ul>
<h4 id="9-6-随机初始化"><a href="#9-6-随机初始化" class="headerlink" title="9.6 随机初始化"></a>9.6 随机初始化</h4><p>初始参数不能全相等(例如zero(n,1),ones(n,1)等)，否则同一层的神经单元值都相同(只需要$\theta_{ij}^{(l)}$的每一行都相同)，进行梯度下降更新时也是相同的。同一层的神经单元依然以相同的函数作为输入来计算。</p>
<p><img src="images/20.png" alt title="随机初始化参数"></p>
<p><strong>$\epsilon_{init}$的选择</strong>：$$\epsilon_{init} = \frac{\sqrt{6}}{\sqrt{s_l+s_{l+1}}}$$，$s_l$和$s_{l+1}$分别代表$l$层和$l+1$层神经元数量。</p>
<h4 id="9-7-神经网络总体过程"><a href="#9-7-神经网络总体过程" class="headerlink" title="9.7 神经网络总体过程"></a>9.7 神经网络总体过程</h4><p>神经网络结构选取：</p>
<p><img src="images/26.png" alt title="神经网络结构选取"></p>
<ol>
<li>选取神经网络的结构<ul>
<li>定义输入单元的数量：特征的维度；</li>
<li>定义输出单元的数量：区分的类别个数；</li>
<li>隐藏层数：一般一个隐藏层，如果隐藏层数大于1，则每个隐藏层有相同的神经单元；</li>
<li>隐藏层的神经单元个数：通常越多越好，每个隐藏层包含的神经单元数量应该和输入$x$的维度向匹配。</li>
</ul>
</li>
<li>训练神经网络<ol>
<li>随机初始化权重；</li>
<li>执行前向传播算法；</li>
<li>计算出代价函数$J(\theta)$;</li>
<li>利用反向传播算法计算代价函数$\frac{\partial }{\partial \theta_{ij}^{(l)}}J(\theta)$;</li>
<li>使用梯度检测，正确后停用梯度检测；</li>
<li>使用梯度下降或者优化梯度下降法最小化代价函数，从而得到参数$\theta$。</li>
</ol>
</li>
</ol>
<hr>
<h3 id="第十章-应用机器学习的建议"><a href="#第十章-应用机器学习的建议" class="headerlink" title="第十章 应用机器学习的建议"></a>第十章 应用机器学习的建议</h3><h4 id="10-1-决定下一步做什么"><a href="#10-1-决定下一步做什么" class="headerlink" title="10.1 决定下一步做什么"></a>10.1 决定下一步做什么</h4><p>学习算法出现大的误差：</p>
<ul>
<li>获取更多的训练样本（解决高方差）；</li>
<li>减少使用特征(防止过拟合)（解决高方差）；</li>
<li>获取、使用更多的特征（解决高偏差）；</li>
<li>增加多项式特征($x_1^2,x_2^2,x_1x_2,$等)（解决高偏差）；</li>
<li>增大正则化参数$\lambda$（解决高方差）;</li>
<li>减小正则化参数$\lambda$（解决高偏差）。</li>
</ul>
<p>机器学习诊断法(machine learning diagnostics)：能够了解算法在哪里出了问题，要想改进一种算法效果的一种测试。</p>
<h4 id="10-2-评估假设"><a href="#10-2-评估假设" class="headerlink" title="10.2 评估假设"></a>10.2 评估假设</h4><p>随机选择70%数据作为训练集，剩余30%作为测试集</p>
<ul>
<li>线性回归$J_{test}(\theta)  = \frac{1}{2m_{test}} \sum_{i=0}^{m_{test}} (h_{\theta}(x_{test}^{(i)})-y_{test}^{(i)})^2$</li>
<li>逻辑回归<ul>
<li>$J_{test}(\theta) = -\frac{1}{m_{test}}[\sum_{i=1}^{m_{test}}y_{test}^{(i)}logh_{\theta}(x_{test}^{(i)})+(1-y_{test}^{(i)})log(h_{\theta}(x_{test}^{(i)}))]$</li>
<li>错误分类(0/1 misclassification error)：$$err(h_{\theta}(x),y)=\begin{cases}<br>1 &amp; \text{ if } h_{\theta}(x)\geqslant0.5,y=0 \<br>1 &amp; \text{ if } h_{\theta}(x)&lt;0.5,y=1 \<br>0 &amp; \text{ otherwise}<br>\end{cases}$$，$$\text{Test error} = \frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err(h_{\theta}(x_{test}^{(i)},y_{test}^{(i)})$$</li>
</ul>
</li>
</ul>
<h4 id="10-3-模型选择和训练、验证、测试集"><a href="#10-3-模型选择和训练、验证、测试集" class="headerlink" title="10.3 模型选择和训练、验证、测试集"></a>10.3 模型选择和训练、验证、测试集</h4><p>模型选择(训练、测试)：</p>
<ol>
<li>通过最小化代价函数$\underset{\theta}{min}J(\theta)$算出第$d$个模型对应的参数$\theta^d$,$d$代表多项式次数；</li>
<li>用得到的$\theta^d$参数在测试集上计算出第$d$个模型对应的代价函数$J_{test}(\theta^{(d)})$，选择最小的代价函数对应的模型；</li>
</ol>
<p>问题：测试集用来训练参数$d$后，又用来对包含模型多项式次数的参数$d$进行测试，导致模型对训练集有很好的泛化能力，但我们需要的是对新样本有很好的泛化能力。</p>
<p>模型选择(训练60%、验证20%、测试20%)：</p>
<ol>
<li>通过最小化代价函数$\underset{\theta}{min}J(\theta)$算出第$d$个模型对应的参数$\theta^d$,$d$代表多项式次数；</li>
<li>用得到的$\theta^d$参数在交叉验证集上计算出第$d$个模型对应的代价函数$J_{cv}(\theta^{(d)})$，选择最小的代价函数对应的模型；</li>
</ol>
<p>通过使用交叉验证集来确定多项式次数$d$，不再用测试集拟合，就可以用测试集来估计算法选出的模型。</p>
<p><img src="images/21.png" alt></p>
<h4 id="10-4-多项式次数与偏差、方差"><a href="#10-4-多项式次数与偏差、方差" class="headerlink" title="10.4 多项式次数与偏差、方差"></a>10.4 多项式次数与偏差、方差</h4><p>训练误差、验证误差与多项式次数的关系：</p>
<p><img src="images/22.png" alt title="训练误差、验证误差与多项式次数的关系"></p>
<p>高偏差(欠拟合)：</p>
<ul>
<li>$J_{train}(\theta)$比较大；</li>
<li>$J_{cv}(\theta) \approx J_{train}(\theta)$.</li>
</ul>
<p>高方差(过拟合)：</p>
<ul>
<li>$J_{train}(\theta)$比较小；</li>
<li>$J_{cv}(\theta) \gg J_{train}(\theta)$.</li>
</ul>
<h4 id="10-5-正则化与偏差、方差的关系"><a href="#10-5-正则化与偏差、方差的关系" class="headerlink" title="10.5 正则化与偏差、方差的关系"></a>10.5 正则化与偏差、方差的关系</h4><p><img src="images/23.png" alt></p>
<ul>
<li><p>$\lambda$过大，除$\theta_0$外，其它参数比较小，模型近似于常数(高偏差、欠拟合)；</p>
</li>
<li><p>$\lambda$过小，容易出现过拟合。</p>
</li>
</ul>
<p>选择正则化参数$\lambda$：</p>
<ol>
<li>以2倍的比例尝试不同的$\lambda$值($\lambda=$0(即不使用正则化),0.01,0.02,…;</li>
<li>通过最小化代价函数$\underset{\theta}{min}J(\theta)$算出第$i$个模型对应的参数$\theta^i$,$i$代表第$i$个$\lambda$参数值；</li>
<li>用得到的$\theta^i$参数在交叉验证集上计算出第$i$个$\lambda$参数对应的代价函数$J_{cv}(\theta^{(i)})$(<strong>此时不包含正则项</strong>)，选择最小的代价函数对应的$\lambda$值作为模型的正则化参数；</li>
</ol>
<h4 id="10-6-学习曲线-Learning-curves"><a href="#10-6-学习曲线-Learning-curves" class="headerlink" title="10.6 学习曲线(Learning curves)"></a>10.6 学习曲线(Learning curves)</h4><p><img src="images/24.png" alt title="高偏差"></p>
<p>如果算法处于高偏差，增加训练集的数量对于改善算法无益。</p>
<p><img src="images/25.png" alt title="高方差"></p>
<p>如果算法处于高方差，增加训练集的数量对于改善算法有较小的帮助。</p>
<hr>
<h3 id="第十一章-机器学习系统设计"><a href="#第十一章-机器学习系统设计" class="headerlink" title="第十一章 机器学习系统设计"></a>第十一章 机器学习系统设计</h3><h4 id="11-1-误差分析"><a href="#11-1-误差分析" class="headerlink" title="11.1 误差分析"></a>11.1 误差分析</h4><ol>
<li>快速建立一个简单的算法，并用验证集测试算法；</li>
<li>绘制学习曲线确定是否需要更多的数据、特征等；</li>
<li>误差分析：手动检查算法在<strong>验证集</strong>上出错的样本，查看这些样本都是什么类别、是否有一些相同的特征。</li>
</ol>
<p>改进算法时，我们要保证对学习算法有一种数值估计方法，也就是说算法能够返回一个数值评价指标来估计算法的执行效果。</p>
<p>快速实施简单粗暴的算法：</p>
<ul>
<li>通过人工分析，识别出主要问题，哪些样本应该花精力去解决；</li>
<li>测试新的想法是否提高算法的效果。</li>
</ul>
<h4 id="11-2-不对称分类的误差评估"><a href="#11-2-不对称分类的误差评估" class="headerlink" title="11.2 不对称分类的误差评估"></a>11.2 不对称分类的误差评估</h4><p>偏斜类(skewed classes)：一个类中的样本数与另一个类的数据相比多很多。通过总预测$y=0$或$y=1$算法可能表现非常好。因此使用分类误差或者分类精度来作为评估度量可能产生一些问题。</p>
<p>查准率/召回率(Precision/Recall)：</p>
<p>$$\text{Precision} = \frac{\text{True positives}}{\text{Predicted positives}} = \frac{\text{True positive}}{\text{True positive} + \text{False positive}}$$(详见西瓜书P30)</p>
<p>$$\text{Recall} = \frac{\text{True positives}}{\text{Actual positives}} = \frac{\text{True positive}}{\text{True positive} + \text{False negative}}$$</p>
<h4 id="11-3-精准度和召回率的权衡"><a href="#11-3-精准度和召回率的权衡" class="headerlink" title="11.3 精准度和召回率的权衡"></a>11.3 精准度和召回率的权衡</h4><p><img src="images/27.png" alt title="查准率和召回率关系"></p>
<p>曲线可能不同，取决于具体的算法。</p>
<p>不同的模型有不同的精准率和召回率，怎么选择评估度量值：</p>
<ul>
<li>看哪个模型有最高的均值$\text{Average} = \frac{\text{Precision} + \text{Recall}}{2}$，当总预测$y=1$时，平均值也很高，故这种方式不可取；</li>
<li>$F_1 \text{Score} = 2\frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$;</li>
</ul>
<p>改变逻辑回归的临界值(如：$h_{\theta}(x) &gt; 0.5$)，可以控制、权衡精准率和召回率，当$F_1$值比较大时，此临界值比较合适。</p>
<h4 id="11-4-机器学习数据"><a href="#11-4-机器学习数据" class="headerlink" title="11.4 机器学习数据"></a>11.4 机器学习数据</h4><p>随着训练集数据的增大, 那些一开始表现地不是很好的算法在最后反而精确度较高。</p>
<p>需要大量的训练数据，而与算法的模型关系不大的情况：</p>
<ul>
<li>有大量的特征来帮助我们准确的预测$y$，相反当特征较少时，过多训练数据对算法性能影响就不那么明显；</li>
<li>需要大量参数的学习算法(神经网络许多隐藏层、许多特征等情况 –&gt;保证里低偏差)，有大量训练集(保证低的方差)就不太可能出现过拟合情况。</li>
</ul>
<p>给定一个输入特征向量$x$，也给定相同的可用信息和学习算法，这个领域的人类专家是否能够准确或者自信到的预测出$y$值。</p>
<pre><code>有足够的特征量以及很多类函数，保证低偏差。
有大量的数据，保证低方差。
这两个条件，可以使你得到一个高性能的学习算法。
</code></pre><p>Andrew总是问自己两个关键测试的问题：</p>
<ol>
<li>当一个专家看到了这些特征量x，他能很有信心的预测出y值吗？</li>
<li>我们能获得庞大的训练数据集吗，并用很多参数去训练学习算法？</li>
</ol>
<hr>
<h3 id="第十二章-支持向量机"><a href="#第十二章-支持向量机" class="headerlink" title="第十二章 支持向量机"></a>第十二章 支持向量机</h3><p>适合复杂的非线性分类器。</p>
<h4 id="12-1-优化目标"><a href="#12-1-优化目标" class="headerlink" title="12.1 优化目标"></a>12.1 优化目标</h4><p>支持向量机也叫大间距分类器。</p>
<p>支持向量机与逻辑回归的代价函数：</p>
<p><img src="images/28.png" alt title="支持向量机与逻辑回归的代价函数"></p>
<p>支持向量机代价函数和图像：</p>
<p><img src="images/31.png" alt title="支持向量机代价函数和图像"></p>
<p>代价函数：$$\underset{\theta}{min}C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{j=1}^{n}\theta_j^2$$</p>
<ul>
<li>(${\color{Red} CA+B}$)当$C$非常大时，要求代价函数尽量小，就需要第一个求和项为0，导致的结果就是学习算法的决策边界会是距离样本距离最大的一条直线；</li>
<li>(${\color{Red} A+\lambda B}$)与正则化参数比较，当正则化参数非常大时，要求最小化代价函数，就需要第二个求和项(所有参数的平方和)为0，即所有参数值几乎为0，就会出现欠拟合现象。</li>
</ul>
<p>假设函数：$$h_{\theta}(x) = \left{\begin{matrix}<br>1 \qquad \text{if } \theta^TX\geqslant0\<br>0 \qquad \text{ otherwise}<br>\end{matrix}\right.$$</p>
<h4 id="12-2-直观上对大间隔的理解"><a href="#12-2-直观上对大间隔的理解" class="headerlink" title="12.2 直观上对大间隔的理解"></a>12.2 直观上对大间隔的理解</h4><p>支持向量机间距：</p>
<p><img src="images/29.png" alt title="支持向量机间距"></p>
<p>如上图，大间距分类器在$C$非常大的情况下，会选择决策边界距离==相似度==(核函数)距离最大的一条直线(图中黑线，非绿线和紫线)。</p>
<p><img src="images/30.png" alt title="支持向量机对异常点敏感"></p>
<p>如上图只用大间距分类器，学习算法会对异常点很敏感(如果$C$设得不是很大，决策边界是黑线；如果$C$设置得很大，则决策边界是紫线)。</p>
<h4 id="12-3-大间隔分类器的数学原理"><a href="#12-3-大间隔分类器的数学原理" class="headerlink" title="12.3 大间隔分类器的数学原理"></a>12.3 大间隔分类器的数学原理</h4><p>范数(长度)$\left | u \right |$：向量$u$的欧几里得长度：$\left | u \right | = \sqrt{u_1^2 + u_2^2 + \cdots + u_n^2}$</p>
<p>$u^T v = P \left | u \right | = u_1 v_1 + u_2 v_2 + \cdots + u_n v_n$，其中$P$为向量$v$在向量$u$上的投影。</p>
<p>参数向量$\theta$与决策边界垂直；</p>
<p>$\theta_0 = 0$意味着决策边界必须通过原点； </p>
<p><strong>最大间距推导</strong>：</p>
<ol>
<li>当$C$非常大时，如果要最小化代价函数，就需要代价函数的第一个求和项尽量等于零，即：$\left{\begin{matrix}<br>p^{(i)} \cdot \left | \theta \right | \geq 1 \qquad \text{if } y^{(i)} = 1\<br>p^{(i)} \cdot \left | \theta \right | \leq -1 \qquad \text{if } y^{(i)} = 0<br>\end{matrix}\right.$,（$p^{(i)}$表示$x^{(i)}$在向量$\theta$上的投影），从而代价函数变为$$\underset{\theta}{min}\frac{1}{2}\sum_{j=1}^{n}\theta_j^2 = \frac{1}{2}\left | \theta \right |^2$$。</li>
</ol>
<p><img src="images/32.png" alt title="最大间距分类推理"></p>
<ol start="2">
<li>推导上面条件成立：<ul>
<li>如上左图，样本在蓝线$\theta$上的投影(红线和粉红险)非常小，即$p^{(i)}$比较小；上面的条件要求$ p^{(i)} \cdot \left | \theta \right | \geq 1$，只能$ \left | \theta \right |$足够大，而与此时的代价函数$$\underset{\theta}{min}\frac{1}{2}\sum_{j=1}^{n}\theta_j^2 = \frac{1}{2}\left | \theta \right |^2$$要尽量小相矛盾；</li>
<li>如上右图，样本在蓝线$\theta$上的投影(红线和粉红险)就达到最大，即$p^{(i)}$比较大；上面的条件要求$ p^{(i)} \cdot \left | \theta \right | \geq 1$，此时 $\left | \theta \right |$就可以比较小，从而与此时的代价函数$$\underset{\theta}{min}\frac{1}{2}\sum_{j=1}^{n}\theta_j^2 = \frac{1}{2}\left | \theta \right |^2$$要尽量小相一致；</li>
</ul>
</li>
</ol>
<h4 id="12-4-核函数"><a href="#12-4-核函数" class="headerlink" title="12.4 核函数"></a>12.4 核函数</h4><p>术语：</p>
<ul>
<li>$f_1,f_2,\cdots,f_m$表示1到$m$个新定义的特征；</li>
<li>$l_1,l_2,\cdots,l_m$表示选择的1到$m$个标记点(landmarks)；</li>
<li>$f_m = Similarity(x,l^{(m)})$相似度函数称为核函数。</li>
</ul>
<p>高斯核函数：$f_m = Similarity(x,l^{(m)}) = \text{exp}(-\frac{\left | x-l^{(m)} \right |^2}{2\delta^2}) = \text{exp}(-\frac{\sum_{j=1}^{n}(x_j - l_j^{(m)})^2}{2\delta^2})$</p>
<ul>
<li><p>如果$x \approx l^{(m)}$：<br> $f_m \approx \text{exp}(-\frac{0^2}{2\delta^2}) \approx 1$</p>
</li>
<li><p>如果$x$离$l^{(m)}$比较远：<br> $f_m \approx \text{exp}(-\frac{(\text{large number})^2}{2\delta^2}) \approx 0$</p>
<p> 故特征$f_m$就是衡量$x$与标记点$l^{(m)}$的相似度(<strong>距离越大，相似度越小</strong>)。</p>
<p> $\delta$越大，同一$x$点与同一标记点的相似度越大。</p>
</li>
</ul>
<p>特征函数：$h_{\theta}(x) = \theta_0 + \theta_1f_1 + \theta_2f_2 + \cdots + \theta_mf_m$</p>
<ul>
<li>$\theta$决定类别与标记点的关系($x$与哪些标记点相似会是正类，与哪些标记点相似是负类)；</li>
<li>根据$\theta$区分的标记点从而画出决策边界。</li>
</ul>
<p>代价函数：</p>
<p>$$\underset{\theta}{min}C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tf^{(i)})+(1-y^{(i)})cost_0(\theta^Tf^{(i)})]+\frac{1}{2}\sum_{j=1}^{n}\theta_j^2$$，$n$为标记点个数</p>
<p>$\sum_{j=1}^{n}\theta_j^2 = \theta^T \theta$</p>
<p>偏差、方差折中($C = \frac{1}{\lambda}$)：</p>
<ul>
<li>$C$比较大，小的偏差，大的方差；</li>
<li>$C$比较小，大的偏差，小的方差；</li>
</ul>
<p>$\delta^2$：</p>
<ul>
<li>较大的$\delta^2$,特征$f_i$非常平滑，高偏差，低方差；</li>
<li>较小的$\delta^2$,特征$f_i$变化非常陡峭，低偏差，高方差；</li>
</ul>
<h4 id="12-5-使用SVM"><a href="#12-5-使用SVM" class="headerlink" title="12.5 使用SVM"></a>12.5 使用SVM</h4><p>选择内核：</p>
<ul>
<li>不使用内核(线性内核)：数据集特征很多，但是数据量比较少，没有足够多的训练集去训练复杂的非线性决策边界；（$n \geqslant m, n = 10,000 , m = 10 \cdots 000$）</li>
<li>高斯内核：$f_i = \text{exp}(-\frac{\left | x-l^{(m)} \right |^2}{2\delta^2})$，需要选择参数$\delta^2$；当数据特征比较少，训练集数量比较大，想要核函数模拟复杂的非线性决策边界时，可以选择高斯内核；（$n = 1 \sim 1000, m = 10 \sim 10 , 000$，而如果$n = 1 \sim 1000, m = 50 , 000^+$这时需要创建或者添加特征，然后用逻辑回归或者不带核函数的SVM）</li>
<li>多项式核函数：$k(x,l) = (x^Tl + \text{constant})^{\text{degree}},(x^Tl)^3,(x^Tl + 1)^2,(x^Tl + 5)^4\cdots$，两个参数(常熟项和指数项)；</li>
</ul>
<p>缩放特征比例：比如计算$\left | x-l^{(m)} \right |^2 = (x_1 - l_1)^2 + (x_2 - l_2)^2 + \cdots + (x_m - l_m)^2$时，特征$x_1,x_2$分别代表房子的面积和卧室数量，则最后的结果会是由卧室面积影响比较大，所以需要缩放特征比例。</p>
<p>默塞尔定理?</p>
<p>逻辑回归和不带核函数的SVM是非常相似的。</p>
<p>SVM是一种凸优化问题，故总可以找到全局最小值。</p>
<hr>
<h3 id="第十三章-无监督学习"><a href="#第十三章-无监督学习" class="headerlink" title="第十三章 无监督学习"></a>第十三章 无监督学习</h3><h4 id="13-1-K-means算法"><a href="#13-1-K-means算法" class="headerlink" title="13.1 K-means算法"></a>13.1 K-means算法</h4><p>K-means算法输入：</p>
<ul>
<li>簇的数量$K$；</li>
<li>训练集{$x^{(1)},x^{(2)},\cdots,x^{(m)}$}。</li>
</ul>
<p><img src="images/33.png" alt title="K-means"></p>
<ol>
<li>随机生成$K$个点，这$K$个点叫聚类中心；</li>
<li>簇分配：遍历所有样本，计算每个样本与哪一个聚类中心更近，来将数据点分配给对应的聚类中心；</li>
<li>移动聚类中心：将聚类中心移动到分配给此聚类中心的所有数据点的均值处；</li>
</ol>
<p>如果某个聚类中心没有一个样本点分配到，则这个聚类中心可以删除。</p>
<h4 id="13-2-优化目标"><a href="#13-2-优化目标" class="headerlink" title="13.2 优化目标"></a>13.2 优化目标</h4><p>失真代价函数：</p>
<p>$$\underset{\begin{matrix}<br>c^{(1)},\cdots,c^{(m)},\<br>u_1,\cdots,u_k<br>\end{matrix}}{min} J(c^{(1)},\cdots,c^{(m)},u_1,\cdots,u_k) = \frac{1}{m}\sum_{i=1}^{m}\left | x^{(i)} - u_{c^{(i)}} \right | ^2$$</p>
<h4 id="13-3-随机初始化"><a href="#13-3-随机初始化" class="headerlink" title="13.3 随机初始化"></a>13.3 随机初始化</h4><ol>
<li>聚类点数应该小于样本数$K&lt;m$；</li>
<li>随机选者$K$个训练样本；</li>
<li>设置$u_1,\cdots,u_k$等于这$K$个样本。</li>
</ol>
<p>局部最优：</p>
<p><img src="images/34.png" alt title="局部最优"></p>
<p>为了尽量避免局部最优化，应该尝试多次(50~1000次)随机初始化(聚类数比较小的情况下$K&lt;10$)：</p>
<ul>
<li>多次随机初始化后运行K-means算法；</li>
<li>然后挑选上面每次运行得到的分类失真函数最小的那一类。</li>
</ul>
<h4 id="13-4-选取聚类数量"><a href="#13-4-选取聚类数量" class="headerlink" title="13.4 选取聚类数量"></a>13.4 选取聚类数量</h4><ol>
<li>肘部法则(Elbow method)：</li>
</ol>
<p><img src="images/35.png" alt title="肘部法则"></p>
<ol start="2">
<li>根据接下来的使用需求决定聚类中心数目。</li>
</ol>
<hr>
<h3 id="第十四章-降维"><a href="#第十四章-降维" class="headerlink" title="第十四章 降维"></a>第十四章 降维</h3><h4 id="14-1-降维的目的"><a href="#14-1-降维的目的" class="headerlink" title="14.1 降维的目的"></a>14.1 降维的目的</h4><ul>
<li><p>目标I：数据压缩(Data Compression)</p>
<p>如果多个特征高度项关，可能就需要降低维数。<br>数据压缩可以让我们节省空间还可以让学习算法运行的更快。</p>
</li>
<li><p>目标II：可视化</p>
</li>
</ul>
<h4 id="14-2-主成分分析-Principal-Component-Analysis，PCA-问题规划"><a href="#14-2-主成分分析-Principal-Component-Analysis，PCA-问题规划" class="headerlink" title="14.2 主成分分析(Principal Component Analysis，PCA)问题规划"></a>14.2 主成分分析(Principal Component Analysis，PCA)问题规划</h4><p>PCA会找一个低维平面(要求投影误差最小)，然后将数据投影在上面。</p>
<p>从$n$维降到$k$维：找到$k$个向量$u^{(1)},u^{(2)},\cdots,u^{(k)}$来投影数据，要求投影误差最小。</p>
<p>找出能够最小化投影距离的方式，来对数据进行投影。</p>
<p>PCA与线性回归：</p>
<p><img src="images/36.png" alt title="PAC与线性回归"></p>
<p>PCA算法过程：</p>
<ol>
<li><p>数据预处理：</p>
<ul>
<li>特征缩放与均值归一化；</li>
</ul>
</li>
<li><p>计算矩阵$U$：</p>
</li>
</ol>
<p><img src="images/37.png" alt title="PAC算法"></p>
<p>​    取矩阵$U$的前$K$列构成新的坐标系$U_{reduce}$。</p>
<ol start="3">
<li>计算降维后的新坐标$z^{(i)} = U_{reduce}^T x^{(i)}$。</li>
</ol>
<h4 id="14-3-主成分数量选择"><a href="#14-3-主成分数量选择" class="headerlink" title="14.3 主成分数量选择"></a>14.3 主成分数量选择</h4><p>平均平方映射误差(Average squared projection error):$\frac{1}{m}\sum_{i=1}^{m}\left | x^{(i)} - x_{approx}^{(i)} \right |^2$，其中$x_{approx}^{(i)}$是低维表面上的映射点；</p>
<p>总变差(Total Variation)：$\frac{1}{m}\sum_{i=1}^{m}\left | x^{(i)} \right |^2$，是每个样本长度的平均值，意味着平均来看训练样本距离零向量多远。</p>
<p>选择$K$值：$$\frac{\frac{1}{m}\sum_{i=1}^{m}\left | x^{(i)} - x_{approx}^{(i)} \right |^2}{\frac{1}{m}\sum_{i=1}^{m}\left | x^{(i)} \right |^2} = 1 - \frac{\sum_{i=1}^{k}S_{ii}}{\sum_{i=1}^nS_{ii}} \leq 0.01 \qquad (\text{1%})$$，保留了99%的差异性。</p>
<p><img src="images/38.png" alt title="确定K值"></p>
<p><img src="images/39.png" alt title="确定K值技巧"></p>
<h4 id="14-5-原始数据的重构"><a href="#14-5-原始数据的重构" class="headerlink" title="14.5 原始数据的重构"></a>14.5 原始数据的重构</h4><p>$z \in \mathbb{R}^k \rightarrow x \in  \mathbb{R}^n$：</p>
<p>$x \approx x_{approx} = u_{reduce} \cdot z$</p>
<h4 id="14-6-应用PCA的建议"><a href="#14-6-应用PCA的建议" class="headerlink" title="14.6 应用PCA的建议"></a>14.6 应用PCA的建议</h4><p>PCA加速学习算法：</p>
<ul>
<li>提取输入：去掉标签的数据集$x^{(1)},x^{(2)},\cdots,x^{(m)} \in \mathbb{R}^n \overset{PCA}{\rightarrow} z^{(1)},z^{(2)},\cdots,z^{(m)} \in \mathbb{R}^k$；</li>
<li>新的数据集：$(z^{(1)},y^{(1)}),(z^{(2)},y^{(2)}),\cdots,(z^{(m)},y^{(m)})$</li>
<li>使用新的数据集去训练模型。</li>
</ul>
<p>PCA定义了一个从$x^{(i)} \rightarrow  z^{(i)}$的映射，此映射是从训练集上得到的(PAC映射的生成过程不能用到测试集和验证集)，但可以用到测试集和交叉验证集上。</p>
<p>PCA的错误使用：防止过拟合。PAC压缩后有更少的特征，就更不可能过拟合。</p>
<ul>
<li>PCA不需要使用标签$y$，仅仅使用输入的$x^{(i)}$去寻找低维数据来近似原本的数据，所以PCA舍掉一些有用信息(减少了数据的维度)而不适合防止过拟合；</li>
<li>正则化代价函数使用了标签$y$，实际是知道$y$的值的所以不会损失掉一些有价值的信息。</li>
</ul>
<p>在实现PCA之前，首先使用原始数据训练模型，当原始数据不能满足需求时才使用PCA。</p>
<hr>
<h3 id="第十五章-异常检测"><a href="#第十五章-异常检测" class="headerlink" title="第十五章 异常检测"></a>第十五章 异常检测</h3><h4 id="15-1-问题动机"><a href="#15-1-问题动机" class="headerlink" title="15.1 问题动机"></a>15.1 问题动机</h4><p>异常检测(Problem motivation)：数据集$x^{(1)},x^{(2)},\cdots,x^{(m)}$，当给定一个新的数据$x_{test}$时，判定它是否异常(与==数据集不同==)。</p>
<p>根据数据集$x^{(1)},x^{(2)},\cdots,x^{(m)}$建立概率模型$p(x)$，$x$正常的概率。</p>
<h4 id="15-2-高斯分布-正态分布"><a href="#15-2-高斯分布-正态分布" class="headerlink" title="15.2 高斯分布(正态分布)"></a>15.2 高斯分布(正态分布)</h4><p>$\sigma$决定了分布曲线的宽度，由于积分为1，故$\sigma$越小，曲线在$x=\mu$处的值就越大。</p>
<p>参数估计：给定数据集估算$\mu$和$\sigma$的值。</p>
<ul>
<li>$\mu = \frac{1}{m}\sum_{i=1}^{m}x^{(i)}$</li>
<li>$\sigma^2 =  \frac{1}{m}\sum_{i=1}^{m}(x^{(i)} - \mu)^2$</li>
</ul>
<h4 id="15-3-算法"><a href="#15-3-算法" class="headerlink" title="15.3 算法"></a>15.3 算法</h4><p>训练集：${x^{(1)},x^{(2)},\cdots,x^{(m)}}$，每个样本$x \in \mathbb{R}^n$</p>
<p>$x_1 \sim N(\mu_1,\sigma_1),x_2 \sim N(\mu_2,\sigma_2),\cdots,x_n \sim N(\mu_n,\sigma_n)$</p>
<p>$x_1$到$x_n$的独立假设：$P(x) = P(x_1;\mu_1,\sigma_1^2)P(x_2;\mu_2,\sigma_2^2) \cdots P(x_n;\mu_n,\sigma_n^2) = \prod_{j=1}^{n}P(x_j;\mu_j,\sigma_j^2)$</p>
<p>异常检测算法：</p>
<p><img src="images/40.png" alt title="异常检测算法"></p>
<h4 id="15-4-开发和评估异常检测系统"><a href="#15-4-开发和评估异常检测系统" class="headerlink" title="15.4 开发和评估异常检测系统"></a>15.4 开发和评估异常检测系统</h4><p><img src="images/41.png" alt title="开发和评估异常检测系统"></p>
<ul>
<li>使用无标签的数据作为训练集，而验证集和测试集是有标签的；(既然数据是标签的，可以用监督学习进行分类)</li>
<li>根据$F_1$分数决定使用哪些特征和$\epsilon$的阈值。</li>
</ul>
<h4 id="15-5-异常检测VS监督学习"><a href="#15-5-异常检测VS监督学习" class="headerlink" title="15.5 异常检测VS监督学习"></a>15.5 异常检测VS监督学习</h4><p><img src="images/42.png" alt title="异常检测VS监督学习"></p>
<p>这里的positive examples是指异常样例。</p>
<h4 id="15-6-选择特征"><a href="#15-6-选择特征" class="headerlink" title="15.6 选择特征"></a>15.6 选择特征</h4><p>在进入异常检测算法之前，先画出数据(用直方图表示)，确保数据近似于正态分布。不是正态分布的可以将数据进行转换为正态分布($log(x_j + c)$，$x_j^\frac{1}{c}$等)或直接使用也可以。</p>
<p>异常检测误差分析：</p>
<p><img src="images/43.png" alt title="正常点和异常点混合在一起"></p>
<ul>
<li>做图绿色的点为异常点，它和正常点混合在一起，训练模型$p(x)$后，评估模型时发现绿色的点检测错误，观察其特征，创造新的特征$x_2$再重新建模，如右图可以正常检测异常。</li>
</ul>
<p>选择特征：</p>
<ul>
<li>选取那些在异常出现时，特征值可能不正常的大或者小的特征；</li>
</ul>
<h4 id="15-7-多变量高斯分布"><a href="#15-7-多变量高斯分布" class="headerlink" title="15.7 多变量高斯分布"></a>15.7 多变量高斯分布</h4><p>$p(x;u,\Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}\left | \Sigma \right |^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}$</p>
<p>$\Sigma$是一个$n \times n$的协方差矩阵，非求和符号。$\Sigma$主对角线$x_{ii}$控制第$i$个特征的宽度，$x_{ij}$控制特征$x_i$与特征$x_j$的关系。</p>
<h4 id="15-8-多变量高斯分布异常检测"><a href="#15-8-多变量高斯分布异常检测" class="headerlink" title="15.8 多变量高斯分布异常检测"></a>15.8 多变量高斯分布异常检测</h4><p>参数估计：</p>
<ul>
<li>$\mu = \frac{1}{m} \sum_{i=1}^{m}x^{(i)}$</li>
<li>$\Sigma = \frac{1}{m} \sum_{i=1}^m(x^{(i)} - \mu)(x^{(i)} - \mu)^T$</li>
</ul>
<p><img src="images/44.png" alt title="多变量高斯分布异常检测"></p>
<p>单变量高斯模型VS多变量高斯模型：</p>
<p><img src="images/45.png" alt title="单变量高斯模型VS多变量高斯模型"></p>
<p>多变量高斯模型转化为单变量高斯模型：手动创建新的特征来描述原特征的关系。</p>
<p>$\Sigma$不可逆：</p>
<ul>
<li>没有满足$m&gt;n$;</li>
<li>存在冗余特征(特征线性相关)。</li>
</ul>
<hr>
<h3 id="第十六章-推荐系统"><a href="#第十六章-推荐系统" class="headerlink" title="第十六章 推荐系统"></a>第十六章 推荐系统</h3><h4 id="16-1-基于内容的推荐算法"><a href="#16-1-基于内容的推荐算法" class="headerlink" title="16.1 基于内容的推荐算法"></a>16.1 基于内容的推荐算法</h4><p>有描述电影内容的特征$x$学习用户的喜爱偏好参数$\theta$情况：</p>
<p><img src="images/46.png" alt title="基于内容的推荐算法"></p>
<h4 id="16-2-协同过滤"><a href="#16-2-协同过滤" class="headerlink" title="16.2 协同过滤"></a>16.2 协同过滤</h4><p>有用户的喜爱偏好参数$\theta$，无电影的特征成分$x$：</p>
<p><img src="images/47.png" alt title="有用户的喜爱偏好，无电影的特征成分"></p>
<ul>
<li>先随机猜想一些$\theta$值来学习不同电影的特征$x$，有这些特征$x$后，就可以基于内容的推荐算法学习更好的$\theta$，新的$\theta$值又可以学习更好的电影特征$x$，一直循环。。。</li>
</ul>
<p>协同过滤算法：当执行算法时要观察大量用户的实际行为来协同的得到更佳的每个人对电影的评分值(每一个用户都在帮助算法更好的进行特征学习)。</p>
<p>同时学习$\theta$和$x$的情况：</p>
<p><img src="images/48.png" alt title="协同过滤算法代价函数"></p>
<ul>
<li><p>式中$(i,j):r(i,j)=1$表示所有有评分的用户和所有有评分的电影进行求和；</p>
</li>
<li><p>不需要在$\theta$和$x$之间反复求；</p>
</li>
<li>$x \in \mathbb{R}^n$,$\theta \in \mathbb{R}^n$,没有截距，因为我们新学习的特征里没有必要将一个特征硬编码为1(即使需要，算法也可以灵活性自主学习使$x_1 = 1$)。</li>
</ul>
<p>协同过滤算法：</p>
<p><img src="images/49.png" alt title="协同过滤算法"></p>
<h4 id="16-3-矢量化：低秩矩阵分解"><a href="#16-3-矢量化：低秩矩阵分解" class="headerlink" title="16.3 矢量化：低秩矩阵分解"></a>16.3 矢量化：低秩矩阵分解</h4><p>低秩(Low-Rank)：如果X是一个m行n列的数值矩阵，rank(X)是X的秩，假如rank (X)远小于m和n，则我们称X是低秩矩阵。低秩矩阵每行或每列都可以用其他的行或列线性表出，可见它包含大量的冗余信息。利用这种冗余信息，可以对缺失数据进行恢复，也可以对数据进行特征提取。</p>
<p>低秩分解：找到两个矩阵，使得两个矩阵的乘积接近原矩阵，并且两个矩阵的乘积的秩低于原矩阵。</p>
<p>低秩逼近：找到一个秩比原矩阵小的矩阵，使得两者接近。</p>
<p><img src="images/50.png" alt title="低秩矩阵分解"></p>
<p>查找相关的产品：</p>
<ul>
<li>只需要第$i$个产品的特征$x_i$与第$j$个产品的特征$x_j$的距离$\left | x_i - x_j \right |$最小;</li>
</ul>
<h4 id="16-4-实现细节：均值归一化"><a href="#16-4-实现细节：均值归一化" class="headerlink" title="16.4 实现细节：均值归一化"></a>16.4 实现细节：均值归一化</h4><p><img src="images/51.png" alt title="均值归一化"></p>
<p>解决了未评分用户使用模型计算时评分结果为0的情况。</p>
<hr>
<h3 id="第十七章-大规模机器学习"><a href="#第十七章-大规模机器学习" class="headerlink" title="第十七章 大规模机器学习"></a>第十七章 大规模机器学习</h3><h4 id="17-1-学习大数据集"><a href="#17-1-学习大数据集" class="headerlink" title="17.1 学习大数据集"></a>17.1 学习大数据集</h4><p>绘制学习曲线，查看是否有使用大量数据集训练的必要，如果较少数据也可以训练，就没有使用大量数据的必要。</p>
<h4 id="17-2-随机梯度下降"><a href="#17-2-随机梯度下降" class="headerlink" title="17.2 随机梯度下降"></a>17.2 随机梯度下降</h4><p>Batch gradient decent：更新$\theta_j$一次以全部的样本数据进行计算;</p>
<p>Stochastic gradient decent：更新$\theta_j$一次以一个样本数据进行计算。</p>
<p>随机梯度下降：</p>
<p><img src="images/52.png" alt title="随机梯度下降"></p>
<p>和批量梯度下降下降的不同：</p>
<ul>
<li>随机梯度下降一个样本就更新了一次$\theta$，批量梯度下降需要遍历所有样本才完成一次$\theta$的更新;</li>
<li>随机梯度下降每一次迭代只考虑能拟合一个样本;</li>
<li>随机梯度下降的到全局最优的曲线如图中的紫红线，与批量梯度下降的红线有所不同。因为<strong>随机梯度下降不是直接收敛到全局最小值,而是在一个范围内反复震荡,最后逐渐接近全局最小值</strong>；</li>
<li>最外层的Repeat是指遍历训练集的次数，一般10次以内就可以了。</li>
</ul>
<h4 id="17-3-min-Batch梯度下降"><a href="#17-3-min-Batch梯度下降" class="headerlink" title="17.3 min-Batch梯度下降"></a>17.3 min-Batch梯度下降</h4><p>三种梯度下降：</p>
<p><img src="images/53.png" alt title="三种梯度下降"></p>
<p>min-Batch梯度下降:</p>
<p><img src="images/54.png" alt title="min-Batch梯度下降"></p>
<ul>
<li>$b$通常取2～100;</li>
<li>通过向量化部分代数库对$b$个样本进行并行运算;</li>
<li>有额外参数$b$,需要确定其大小。</li>
</ul>
<h4 id="17-4-随机梯度下降收敛"><a href="#17-4-随机梯度下降收敛" class="headerlink" title="17.4 随机梯度下降收敛"></a>17.4 随机梯度下降收敛</h4><p>批量梯度下降：</p>
<ul>
<li>每一次梯度下降迭代计算一次代价函数,画出曲线；</li>
</ul>
<p>随机梯度下降：</p>
<ul>
<li>在更新$\theta$之前使用$(x^{(i)},y^{(i)})$计算$cost(\theta,(x^{(i)},y^{(i)}))$。每1000次迭代(假如)取$cost(\theta,(x^{(i)},y^{(i)}))$的平均值作为一个点画出图像；</li>
</ul>
<p><img src="images/55.png" alt title="随机梯度下降的学习曲线"></p>
<ul>
<li>左上图红色是比蓝色更小的学习率画的曲线，更加平滑,收敛效果也较好一些；</li>
<li>右上图蓝线是1000次迭代取平均值作为一点，红线是以5000次迭代取平均值画出的图像。5000次的更加平滑，但是有较高的延时性；</li>
<li>左下图蓝线一直振荡,看不出是否收敛，加大迭代次数后红线看出是在收敛，证明蓝线的迭代次数取得太小，从而振荡。如果粉红线是采用比较高的迭代次数画出，那么算法没有进行学习，需要调整学习率或者特征；</li>
<li>右下图是算法发散的信号，此时学要使用更小的学习率。</li>
</ul>
<p>随机梯度下降的学习率$\alpha$一般是一个不变的常数,如果想梯度下降更好的收敛到全局最小值，可以==让学习率随时间变化逐渐减小==(如：$\alpha = \frac{\text{const1}}{\text{iterationNumber + const2}}$)。</p>
<h4 id="17-5-在线学习"><a href="#17-5-在线学习" class="headerlink" title="17.5 在线学习"></a>17.5 在线学习</h4><p>有连续的数据流，就没必要将数据保存在固定的数据集中，因为连续的数据流可以为我们提供不断的新的数据供算法学习。</p>
<p>在线学习可以适应变化的用户偏好。</p>
<h4 id="17-6-MapReduce与数据并行"><a href="#17-6-MapReduce与数据并行" class="headerlink" title="17.6 MapReduce与数据并行"></a>17.6 MapReduce与数据并行</h4><p><img src="images/56.png" alt title="MapReduce"></p>
<hr>
<h3 id="第十八章-应用举例：照片OCR"><a href="#第十八章-应用举例：照片OCR" class="headerlink" title="第十八章 应用举例：照片OCR"></a>第十八章 应用举例：照片OCR</h3><h4 id="18-1-问题描述"><a href="#18-1-问题描述" class="headerlink" title="18.1 问题描述"></a>18.1 问题描述</h4><p>照片光学字符识别(Photo Optical Character Recognition)：识别照片中的文字信息。</p>
<p>Photo OCR pipeline:</p>
<ol>
<li>找到文字区域；</li>
<li>文字分割：将单词分割为单个字母；</li>
<li>拼写校正；</li>
<li>字母分类。</li>
</ol>
<p><img src="images/57.png&#39;Photo OCR pipeline&#39;" alt></p>
<p>Pipeline设计：将问题分为不同的模块；</p>
<h4 id="18-2-滑动窗口"><a href="#18-2-滑动窗口" class="headerlink" title="18.2 滑动窗口"></a>18.2 滑动窗口</h4><p>Slide window：</p>
<ol>
<li>选取合适大小的window;</li>
<li>选取合适的步长移动window；</li>
<li>判断window中是否有解决问题的情况；</li>
<li>考虑是否需要以更大window进行搜索；</li>
<li>将获取的结果标识出来；</li>
<li>过滤掉不正常的结果(如：文字中宽度大于长度);</li>
<li>输出结果。</li>
</ol>
<h4 id="18-3-获取大量数据：人工数据"><a href="#18-3-获取大量数据：人工数据" class="headerlink" title="18.3 获取大量数据：人工数据"></a>18.3 获取大量数据：人工数据</h4><ol>
<li>从零开始生成数据；</li>
<li>基于现在的样本引入有意义的失真(introduce distortion)；</li>
<li>自己收集数据然后添加标签；</li>
<li>众包。</li>
</ol>
<p>在考虑获取更多数据之前，应该检查分类器是否<strong>偏差比较低</strong>；</p>
<p><strong>如果获取现在10倍的数据需要做多少的工作</strong>。</p>
<h4 id="18-4-天花板分析：如何分配时间"><a href="#18-4-天花板分析：如何分配时间" class="headerlink" title="18.4 天花板分析：如何分配时间"></a>18.4 天花板分析：如何分配时间</h4><p>确定哪一个模块对整个模型最后的准确率影响最大(该模块花更多时间)：</p>
<ol>
<li>从前向后依次确保每一个模块的工作输出正确率100%(如：OCR的文字区域检查，通过手动输入准确的文字区域)，其它模块正常运行，然后查看整个模型的最终正确率是否提高；</li>
</ol>
<p><img src="images/58.png&#39;上限分析&#39;" alt></p>
<p>机器学习不要凭直觉判断 </p>
<hr>
<h3 id="第十九章-总结"><a href="#第十九章-总结" class="headerlink" title="第十九章 总结"></a>第十九章 总结</h3><p><img src="images/59.png&#39;总结&#39;" alt></p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/04/21/my-password/" rel="next" title="Hello World">
                <i class="fa fa-chevron-left"></i> Hello World
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        
          <ul class="sidebar-nav motion-element">
            <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
              Table of Contents
            </li>
            <li class="sidebar-nav-overview" data-target="site-overview-wrap">
              Overview
            </li>
          </ul>
        
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/killer.jpg" alt="wuji">
            
              <p class="site-author-name" itemprop="name">wuji</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">5</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
        
        <!--noindex-->
          <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#第一章-初识机器学习"><span class="nav-number">1.</span> <span class="nav-text">第一章 初识机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1-机器学习与生活"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 机器学习与生活</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-2-什么是机器学习"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 什么是机器学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-3-监督学习-Supervised-Learning"><span class="nav-number">1.3.</span> <span class="nav-text">1.3 监督学习(Supervised Learning)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-4-无监督学习-Unsupervised-Learning"><span class="nav-number">1.4.</span> <span class="nav-text">1.4 无监督学习(Unsupervised Learning)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第二章-单变量线性回归"><span class="nav-number">2.</span> <span class="nav-text">第二章 单变量线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-1-模型描述"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 模型描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-代价函数"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-梯度下降"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 梯度下降</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第三章-线性代数回顾"><span class="nav-number">3.</span> <span class="nav-text">第三章 线性代数回顾</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-矩阵和向量"><span class="nav-number">3.1.</span> <span class="nav-text">3.1 矩阵和向量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-加法和标量乘法"><span class="nav-number">3.2.</span> <span class="nav-text">3.2 加法和标量乘法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-矩阵乘法"><span class="nav-number">3.3.</span> <span class="nav-text">3.3 矩阵乘法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-逆和转置"><span class="nav-number">3.4.</span> <span class="nav-text">3.4 逆和转置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第四章-多变量线性回归"><span class="nav-number">4.</span> <span class="nav-text">第四章 多变量线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#4-1-多元或多变量"><span class="nav-number">4.1.</span> <span class="nav-text">4.1 多元或多变量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-2-多元梯度下降法"><span class="nav-number">4.2.</span> <span class="nav-text">4.2 多元梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-1-定义"><span class="nav-number">4.2.1.</span> <span class="nav-text">4.2.1 定义</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-2-特征缩放-Feature-Scaling"><span class="nav-number">4.2.2.</span> <span class="nav-text">4.2.2 特征缩放(Feature Scaling)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-3-学习率"><span class="nav-number">4.2.3.</span> <span class="nav-text">4.2.3 学习率</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-4-特征和多项式回归-Polynomial-Regression"><span class="nav-number">4.2.4.</span> <span class="nav-text">4.2.4 特征和多项式回归(Polynomial Regression)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-5-正规方程Normal-Equations（区别与迭代方法的直接法）"><span class="nav-number">4.2.5.</span> <span class="nav-text">4.2.5 正规方程Normal Equations（区别与迭代方法的直接法）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#4-2-6-正规方在矩阵不可逆的情况下的解法"><span class="nav-number">4.2.6.</span> <span class="nav-text">4.2.6 正规方在矩阵不可逆的情况下的解法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第五章-Octave-Matlab-教程"><span class="nav-number">5.</span> <span class="nav-text">第五章 Octave/Matlab 教程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#5-1-逻辑运算"><span class="nav-number">5.1.</span> <span class="nav-text">5.1 逻辑运算</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-2-Octave常用命令"><span class="nav-number">5.2.</span> <span class="nav-text">5.2 Octave常用命令</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-3-矩阵"><span class="nav-number">5.3.</span> <span class="nav-text">5.3 矩阵</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-4-数据操作"><span class="nav-number">5.4.</span> <span class="nav-text">5.4 数据操作</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-5-数据绘制"><span class="nav-number">5.5.</span> <span class="nav-text">5.5 数据绘制</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-6-控制语句"><span class="nav-number">5.6.</span> <span class="nav-text">5.6 控制语句</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-6-函数调用"><span class="nav-number">5.7.</span> <span class="nav-text">5.6 函数调用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-7-矢量"><span class="nav-number">5.8.</span> <span class="nav-text">5.7 矢量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第六章-Logistic回归"><span class="nav-number">6.</span> <span class="nav-text">第六章 Logistic回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-1-分类-不能直接使用线性回归的原因"><span class="nav-number">6.1.</span> <span class="nav-text">6.1 分类(不能直接使用线性回归的原因)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-假设函数"><span class="nav-number">6.2.</span> <span class="nav-text">6.2 假设函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-决策边界-decision-boundary"><span class="nav-number">6.3.</span> <span class="nav-text">6.3 决策边界(decision boundary)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-4-代价函数"><span class="nav-number">6.4.</span> <span class="nav-text">6.4 代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-5-简化代价函数与梯度下降"><span class="nav-number">6.5.</span> <span class="nav-text">6.5 简化代价函数与梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-6-高级优化"><span class="nav-number">6.6.</span> <span class="nav-text">6.6 高级优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-7-多元分类：一对多"><span class="nav-number">6.7.</span> <span class="nav-text">6.7 多元分类：一对多</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第七章-正则化-Regularization"><span class="nav-number">7.</span> <span class="nav-text">第七章 正则化(Regularization)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-过拟合问题"><span class="nav-number">7.1.</span> <span class="nav-text">7.1 过拟合问题</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-含正则化项的代价函数"><span class="nav-number">7.2.</span> <span class="nav-text">7.2 含正则化项的代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-4-线性回归正则化"><span class="nav-number">7.3.</span> <span class="nav-text">7.4 线性回归正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-5-logistic回归正则化"><span class="nav-number">7.4.</span> <span class="nav-text">7.5 logistic回归正则化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第八章-神经网络学习"><span class="nav-number">8.</span> <span class="nav-text">第八章 神经网络学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#8-1-非线性假设"><span class="nav-number">8.1.</span> <span class="nav-text">8.1 非线性假设</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-2-神经元与大脑"><span class="nav-number">8.2.</span> <span class="nav-text">8.2 神经元与大脑</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#8-3-神经网络模型"><span class="nav-number">8.3.</span> <span class="nav-text">8.3 神经网络模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第九章-神经网络参数的反向传播算法"><span class="nav-number">9.</span> <span class="nav-text">第九章 神经网络参数的反向传播算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-1-代价函数"><span class="nav-number">9.1.</span> <span class="nav-text">9.1 代价函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-反向传播算法-Backpropagation-algorithm"><span class="nav-number">9.2.</span> <span class="nav-text">9.2 反向传播算法(Backpropagation algorithm)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-3-理解反向传播算法"><span class="nav-number">9.3.</span> <span class="nav-text">9.3 理解反向传播算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-4-使用注意：展开参数"><span class="nav-number">9.4.</span> <span class="nav-text">9.4 使用注意：展开参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-5-梯度检测"><span class="nav-number">9.5.</span> <span class="nav-text">9.5 梯度检测</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-6-随机初始化"><span class="nav-number">9.6.</span> <span class="nav-text">9.6 随机初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-7-神经网络总体过程"><span class="nav-number">9.7.</span> <span class="nav-text">9.7 神经网络总体过程</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第十章-应用机器学习的建议"><span class="nav-number">10.</span> <span class="nav-text">第十章 应用机器学习的建议</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#10-1-决定下一步做什么"><span class="nav-number">10.1.</span> <span class="nav-text">10.1 决定下一步做什么</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-2-评估假设"><span class="nav-number">10.2.</span> <span class="nav-text">10.2 评估假设</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-3-模型选择和训练、验证、测试集"><span class="nav-number">10.3.</span> <span class="nav-text">10.3 模型选择和训练、验证、测试集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-4-多项式次数与偏差、方差"><span class="nav-number">10.4.</span> <span class="nav-text">10.4 多项式次数与偏差、方差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-5-正则化与偏差、方差的关系"><span class="nav-number">10.5.</span> <span class="nav-text">10.5 正则化与偏差、方差的关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#10-6-学习曲线-Learning-curves"><span class="nav-number">10.6.</span> <span class="nav-text">10.6 学习曲线(Learning curves)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第十一章-机器学习系统设计"><span class="nav-number">11.</span> <span class="nav-text">第十一章 机器学习系统设计</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#11-1-误差分析"><span class="nav-number">11.1.</span> <span class="nav-text">11.1 误差分析</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-2-不对称分类的误差评估"><span class="nav-number">11.2.</span> <span class="nav-text">11.2 不对称分类的误差评估</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-3-精准度和召回率的权衡"><span class="nav-number">11.3.</span> <span class="nav-text">11.3 精准度和召回率的权衡</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#11-4-机器学习数据"><span class="nav-number">11.4.</span> <span class="nav-text">11.4 机器学习数据</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第十二章-支持向量机"><span class="nav-number">12.</span> <span class="nav-text">第十二章 支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#12-1-优化目标"><span class="nav-number">12.1.</span> <span class="nav-text">12.1 优化目标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-2-直观上对大间隔的理解"><span class="nav-number">12.2.</span> <span class="nav-text">12.2 直观上对大间隔的理解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-3-大间隔分类器的数学原理"><span class="nav-number">12.3.</span> <span class="nav-text">12.3 大间隔分类器的数学原理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-4-核函数"><span class="nav-number">12.4.</span> <span class="nav-text">12.4 核函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#12-5-使用SVM"><span class="nav-number">12.5.</span> <span class="nav-text">12.5 使用SVM</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第十三章-无监督学习"><span class="nav-number">13.</span> <span class="nav-text">第十三章 无监督学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#13-1-K-means算法"><span class="nav-number">13.1.</span> <span class="nav-text">13.1 K-means算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-2-优化目标"><span class="nav-number">13.2.</span> <span class="nav-text">13.2 优化目标</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-3-随机初始化"><span class="nav-number">13.3.</span> <span class="nav-text">13.3 随机初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#13-4-选取聚类数量"><span class="nav-number">13.4.</span> <span class="nav-text">13.4 选取聚类数量</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第十四章-降维"><span class="nav-number">14.</span> <span class="nav-text">第十四章 降维</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#14-1-降维的目的"><span class="nav-number">14.1.</span> <span class="nav-text">14.1 降维的目的</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-2-主成分分析-Principal-Component-Analysis，PCA-问题规划"><span class="nav-number">14.2.</span> <span class="nav-text">14.2 主成分分析(Principal Component Analysis，PCA)问题规划</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-3-主成分数量选择"><span class="nav-number">14.3.</span> <span class="nav-text">14.3 主成分数量选择</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-5-原始数据的重构"><span class="nav-number">14.4.</span> <span class="nav-text">14.5 原始数据的重构</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#14-6-应用PCA的建议"><span class="nav-number">14.5.</span> <span class="nav-text">14.6 应用PCA的建议</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第十五章-异常检测"><span class="nav-number">15.</span> <span class="nav-text">第十五章 异常检测</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#15-1-问题动机"><span class="nav-number">15.1.</span> <span class="nav-text">15.1 问题动机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-2-高斯分布-正态分布"><span class="nav-number">15.2.</span> <span class="nav-text">15.2 高斯分布(正态分布)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-3-算法"><span class="nav-number">15.3.</span> <span class="nav-text">15.3 算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-4-开发和评估异常检测系统"><span class="nav-number">15.4.</span> <span class="nav-text">15.4 开发和评估异常检测系统</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-5-异常检测VS监督学习"><span class="nav-number">15.5.</span> <span class="nav-text">15.5 异常检测VS监督学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-6-选择特征"><span class="nav-number">15.6.</span> <span class="nav-text">15.6 选择特征</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-7-多变量高斯分布"><span class="nav-number">15.7.</span> <span class="nav-text">15.7 多变量高斯分布</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#15-8-多变量高斯分布异常检测"><span class="nav-number">15.8.</span> <span class="nav-text">15.8 多变量高斯分布异常检测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第十六章-推荐系统"><span class="nav-number">16.</span> <span class="nav-text">第十六章 推荐系统</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#16-1-基于内容的推荐算法"><span class="nav-number">16.1.</span> <span class="nav-text">16.1 基于内容的推荐算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-2-协同过滤"><span class="nav-number">16.2.</span> <span class="nav-text">16.2 协同过滤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-3-矢量化：低秩矩阵分解"><span class="nav-number">16.3.</span> <span class="nav-text">16.3 矢量化：低秩矩阵分解</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#16-4-实现细节：均值归一化"><span class="nav-number">16.4.</span> <span class="nav-text">16.4 实现细节：均值归一化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第十七章-大规模机器学习"><span class="nav-number">17.</span> <span class="nav-text">第十七章 大规模机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#17-1-学习大数据集"><span class="nav-number">17.1.</span> <span class="nav-text">17.1 学习大数据集</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-2-随机梯度下降"><span class="nav-number">17.2.</span> <span class="nav-text">17.2 随机梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-3-min-Batch梯度下降"><span class="nav-number">17.3.</span> <span class="nav-text">17.3 min-Batch梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-4-随机梯度下降收敛"><span class="nav-number">17.4.</span> <span class="nav-text">17.4 随机梯度下降收敛</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-5-在线学习"><span class="nav-number">17.5.</span> <span class="nav-text">17.5 在线学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#17-6-MapReduce与数据并行"><span class="nav-number">17.6.</span> <span class="nav-text">17.6 MapReduce与数据并行</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第十八章-应用举例：照片OCR"><span class="nav-number">18.</span> <span class="nav-text">第十八章 应用举例：照片OCR</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#18-1-问题描述"><span class="nav-number">18.1.</span> <span class="nav-text">18.1 问题描述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#18-2-滑动窗口"><span class="nav-number">18.2.</span> <span class="nav-text">18.2 滑动窗口</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#18-3-获取大量数据：人工数据"><span class="nav-number">18.3.</span> <span class="nav-text">18.3 获取大量数据：人工数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#18-4-天花板分析：如何分配时间"><span class="nav-number">18.4.</span> <span class="nav-text">18.4 天花板分析：如何分配时间</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#第十九章-总结"><span class="nav-number">19.</span> <span class="nav-text">第十九章 总结</span></a></li></ol></div>
            

          </div>
        </div>
        <!--/noindex-->
        
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">wuji</span>

  

  
</div>









        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/utils.js?v=7.1.0"></script>

  <script src="/js/motion.js?v=7.1.0"></script>



  
  


  <script src="/js/affix.js?v=7.1.0"></script>

  <script src="/js/schemes/pisces.js?v=7.1.0"></script>



  
  <script src="/js/scrollspy.js?v=7.1.0"></script>
<script src="/js/post-details.js?v=7.1.0"></script>



  


  <script src="/js/next-boot.js?v=7.1.0"></script>


  

  

  

  


  


  




  

  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });
  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') { next = next.nextSibling }
        if (next && next.nodeName.toLowerCase() === 'br') { next.parentNode.removeChild(next) }
      }
    });
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      document.getElementById(all[i].inputID + '-Frame').parentNode.className += ' has-jax';
    }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
